<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>大模型-分词器</title>
    <link href="/tech-blog/2025/04/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%88%86%E8%AF%8D%E5%99%A8/"/>
    <url>/tech-blog/2025/04/03/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%88%86%E8%AF%8D%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h1><h2 id="基于规则"><a href="#基于规则" class="headerlink" title="基于规则"></a>基于规则</h2><h2 id="基于统计"><a href="#基于统计" class="headerlink" title="基于统计"></a>基于统计</h2><h3 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h3><h3 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte-Pair Encoding(BPE)"></a>Byte-Pair Encoding(BPE)</h3><p>tiktoken是OpenAI实现BPE方法。</p><h3 id="SentencePiece"><a href="#SentencePiece" class="headerlink" title="SentencePiece"></a>SentencePiece</h3>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/tech-blog/2025/04/03/001-%E6%8A%95%E8%B5%84-%E5%9F%BA%E6%9C%AC%E7%AD%96%E7%95%A5/"/>
    <url>/tech-blog/2025/04/03/001-%E6%8A%95%E8%B5%84-%E5%9F%BA%E6%9C%AC%E7%AD%96%E7%95%A5/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>你的大脑也许根本就不会老：认知能力与年龄的复杂关系</title>
    <link href="/tech-blog/2025/04/02/%E8%84%91%E7%A7%91%E5%AD%A6-%E5%B9%B4%E9%BE%84%E4%B8%8E%E8%AE%A4%E7%9F%A5%E8%83%BD%E5%8A%9B/"/>
    <url>/tech-blog/2025/04/02/%E8%84%91%E7%A7%91%E5%AD%A6-%E5%B9%B4%E9%BE%84%E4%B8%8E%E8%AE%A4%E7%9F%A5%E8%83%BD%E5%8A%9B/</url>
    
    <content type="html"><![CDATA[<h1 id="你的大脑也许根本就不会老：认知能力与年龄的复杂关系"><a href="#你的大脑也许根本就不会老：认知能力与年龄的复杂关系" class="headerlink" title="你的大脑也许根本就不会老：认知能力与年龄的复杂关系"></a>你的大脑也许根本就不会老：认知能力与年龄的复杂关系</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>人们普遍认为，随着年龄增长，认知能力会不可避免地下降。这种观念已深入人心：30岁后，大脑开始走下坡路，记忆力减退、学习新事物变得困难、思维速度放缓。然而，来自德国的一项最新研究似乎颠覆了这一长期以来的传统观念。这项研究表明，认知能力的衰退可能并非年龄的必然结果，而更多地取决于一个简单却深刻的原则：<strong>使用它，否则就会失去它</strong>。</p><h2 id="研究概述"><a href="#研究概述" class="headerlink" title="研究概述"></a>研究概述</h2><p>德国研究团队对国际成人能力评估项目(PIAAC-L)的数据进行了分析，该研究跟踪考察了3,000多名16-65岁的德国成年人，每隔3.5年测试一次他们的读写和数学能力。研究结果挑战了认知技能（特别是读写和计算能力）从30岁左右开始下降的传统观点。</p><h2 id="关键发现"><a href="#关键发现" class="headerlink" title="关键发现"></a>关键发现</h2><h3 id="1-平均趋势与年龄拐点"><a href="#1-平均趋势与年龄拐点" class="headerlink" title="1. 平均趋势与年龄拐点"></a>1. 平均趋势与年龄拐点</h3><p>研究显示，在总体人群中：</p><ul><li><strong>读写能力</strong>在46岁左右达到顶峰，之后下降幅度相对较小</li><li><strong>计算能力</strong>在41岁左右达到顶峰，之后下降趋势更为明显</li></ul><p><img src="https://example.com/cognitive_age_chart.jpg" alt="认知能力与年龄关系图"></p><h3 id="2-技能水平差异显著影响衰退模式"><a href="#2-技能水平差异显著影响衰退模式" class="headerlink" title="2. 技能水平差异显著影响衰退模式"></a>2. 技能水平差异显著影响衰退模式</h3><p>研究中最引人注目的发现是，高技能群体和低技能群体在认知能力随年龄变化的模式上存在显著差异：</p><ul><li><strong>高技能群体</strong>：即使超过40岁，认知技能仍无明显下降趋势，有些人甚至继续呈现增长或保持稳定状态</li><li><strong>低技能群体</strong>（体力或脑力劳动密集型工作者）：从30多岁中期开始就出现了明显的技能下降</li></ul><p>值得注意的是，<strong>工作环境中的技能使用频率</strong>对技能维持的影响大于家庭环境中的使用。</p><h3 id="3-职业与教育水平的影响"><a href="#3-职业与教育水平的影响" class="headerlink" title="3. 职业与教育水平的影响"></a>3. 职业与教育水平的影响</h3><p>研究进一步揭示了职业类型和教育水平对认知能力维持的重要影响：</p><ul><li><p><strong>白领工作者和高学历者</strong>：</p><ul><li>若在日常工作中频繁使用认知技能，其能力随年龄增长而提升或至少维持稳定</li><li>若使用频率低，则技能仍会出现下降</li></ul></li><li><p><strong>蓝领工作者和低学历者</strong>：</p><ul><li>除非在工作中高频率使用认知技能，否则技能会随年龄增长而明显下降</li><li>技能使用的”保护作用”在这一群体中尤为重要</li></ul></li></ul><h3 id="4-性别差异"><a href="#4-性别差异" class="headerlink" title="4. 性别差异"></a>4. 性别差异</h3><p>研究还发现了认知能力衰退的性别差异：</p><ul><li><strong>女性</strong>在老年阶段（尤其是40-65岁）的技能下降幅度大于男性，特别是在计算能力上</li><li>这一差异可能与社会角色、职业选择和技能使用机会等多种因素有关</li></ul><h2 id="深度分析：”使用它，否则就会失去它”"><a href="#深度分析：”使用它，否则就会失去它”" class="headerlink" title="深度分析：”使用它，否则就会失去它”"></a>深度分析：”使用它，否则就会失去它”</h2><p>这项研究的结果与现代心理学和神经科学的多项发现高度一致，强化了”使用它，否则就会失去它”的原则在认知保持中的核心地位。</p><h3 id="神经可塑性与认知保留"><a href="#神经可塑性与认知保留" class="headerlink" title="神经可塑性与认知保留"></a>神经可塑性与认知保留</h3><p>大脑的神经可塑性（即神经网络根据经验重组的能力）在整个生命周期内持续存在。研究结果支持这一观点，表明：</p><ol><li><strong>持续的认知挑战</strong>可以维持和加强神经连接</li><li><strong>认知储备</strong>(Cognitive Reserve)理论得到支持：频繁的脑力活动建立更多神经通路，提供对认知衰退的”缓冲”</li><li><strong>活动依赖性可塑性</strong>(Activity-Dependent Plasticity)：经常使用的神经通路更不容易衰退</li></ol><h3 id="社会经济因素的复杂影响"><a href="#社会经济因素的复杂影响" class="headerlink" title="社会经济因素的复杂影响"></a>社会经济因素的复杂影响</h3><p>研究结果揭示了社会经济因素如何塑造认知轨迹：</p><ul><li><strong>职业环境</strong>作为认知刺激的主要来源，对认知能力的保持起着关键作用</li><li><strong>教育水平</strong>不仅提供基础认知技能，还可能影响个体追求认知挑战的倾向</li><li><strong>认知活动获取的机会不平等</strong>可能放大社会经济差距带来的健康不平等</li></ul><h2 id="实际应用：保持大脑敏锐的策略"><a href="#实际应用：保持大脑敏锐的策略" class="headerlink" title="实际应用：保持大脑敏锐的策略"></a>实际应用：保持大脑敏锐的策略</h2><p>基于这些发现，以下策略可能有助于维持认知能力：</p><h3 id="职场策略"><a href="#职场策略" class="headerlink" title="职场策略"></a>职场策略</h3><ul><li>寻求需要持续学习和解决问题的工作职责</li><li>即使在常规工作中，也要主动增加认知挑战</li><li>追求职业发展和技能拓展的机会</li></ul><h3 id="个人生活策略"><a href="#个人生活策略" class="headerlink" title="个人生活策略"></a>个人生活策略</h3><ul><li>培养需要持续脑力活动的兴趣爱好（阅读、学习新语言、乐器、战略游戏等）</li><li>保持社交互动，参与具有认知挑战性的讨论和活动</li><li>将计算、阅读等认知活动融入日常生活</li></ul><h3 id="社会政策含义"><a href="#社会政策含义" class="headerlink" title="社会政策含义"></a>社会政策含义</h3><ul><li>创造终身学习的机会，特别是针对低技能职业的工作者</li><li>为老年人设计认知挑战项目，鼓励持续的脑力活动</li><li>减少获取认知刺激机会的社会经济障碍</li></ul><h2 id="结论：重新认识脑老化"><a href="#结论：重新认识脑老化" class="headerlink" title="结论：重新认识脑老化"></a>结论：重新认识脑老化</h2><p>这项研究的结果对我们理解认知老化具有深远影响。它表明，认知能力的”自然”衰退轨迹可能被严重高估了，而环境因素和个人行为的影响则被低估。</p><p><strong>大脑衰老的过程远比我们想象的更为复杂且可干预</strong>。虽然生物学老化确实存在，但我们对自身认知命运的掌控能力可能比传统观念认为的要大得多。认知衰退很大程度上取决于我们如何使用大脑，而不仅仅是年龄本身。</p><p>正如研究所示，年龄带来的并非必然是衰退，而是选择的岔路口：持续挑战和使用大脑可以维持甚至提升认知能力，而认知懈怠则可能加速能力下降。这一发现不仅为个人提供了积极应对老龄化的策略，也为社会如何更好地支持健康脑老化提供了方向。</p><p>最终，这项研究向我们传递了一个令人鼓舞的信息：也许你的大脑根本就不会老，它只是需要你多用而已！</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.science.org/doi/10.1126/sciadv.ads1560">Age and cognitive skills: Use it or lose it</a></p>]]></content>
    
    
    <categories>
      
      <category>脑科学</category>
      
      <category>认知心理学</category>
      
    </categories>
    
    
    <tags>
      
      <tag>脑科学</tag>
      
      <tag>认知能力</tag>
      
      <tag>神经可塑性</tag>
      
      <tag>生命科学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型训练</title>
    <link href="/tech-blog/2025/04/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <url>/tech-blog/2025/04/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="强化学习在大模型中的核心作用"><a href="#强化学习在大模型中的核心作用" class="headerlink" title="强化学习在大模型中的核心作用"></a>强化学习在大模型中的核心作用</h1><p>显著提升大模型的推理能力。</p><h2 id="代表性技术"><a href="#代表性技术" class="headerlink" title="代表性技术"></a>代表性技术</h2><h3 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h3><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="研究点"><a href="#研究点" class="headerlink" title="研究点"></a>研究点</h2><h3 id="提升模型推理能力"><a href="#提升模型推理能力" class="headerlink" title="提升模型推理能力"></a>提升模型推理能力</h3><h3 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h3><p>现状：预先确定了工具的使用模式、限制了对最优策略的探索、实现透明度不足等。<br>Tool-Integrated Reinforcement Learning框架允许模型直接从基座模型开始，通过强化学习自主探索最优工具使用策略，而非受限于与定义的工具使用模式。</p><p>参考<br>Fine-Tuning Language Models from Human Preferences<br><a href="https://mp.weixin.qq.com/s/TLQ3TdrB5gLb697AFmjEYQ">ChatGPT 背后的“功臣”——RLHF 技术详解</a><br><a href="">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a><br><a href="https://arxiv.org/pdf/2503.23383">TORL: Scaling Tool-Integrated RL</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型微调实战</title>
    <link href="/tech-blog/2025/04/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/"/>
    <url>/tech-blog/2025/04/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<h1 id="微调医疗问答大模型"><a href="#微调医疗问答大模型" class="headerlink" title="微调医疗问答大模型"></a>微调医疗问答大模型</h1>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型训练</title>
    <link href="/tech-blog/2025/04/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    <url>/tech-blog/2025/04/01/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E9%A2%84%E8%AE%AD%E7%BB%83/</url>
    
    <content type="html"><![CDATA[<h1 id="Scaling-Law"><a href="#Scaling-Law" class="headerlink" title="Scaling Law"></a>Scaling Law</h1><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><h2 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h2><h2 id="DeepSeek"><a href="#DeepSeek" class="headerlink" title="DeepSeek"></a>DeepSeek</h2><h2 id="ChatGLM"><a href="#ChatGLM" class="headerlink" title="ChatGLM"></a>ChatGLM</h2><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="实效性低"><a href="#实效性低" class="headerlink" title="实效性低"></a>实效性低</h2><p>通过RAG等技术解决。</p><h2 id="大模型幻觉"><a href="#大模型幻觉" class="headerlink" title="大模型幻觉"></a>大模型幻觉</h2><p>生成式模型生成了与模型输入不相关或不准确的内容。<br>缓解大模型幻觉的常用策略</p><h1 id="提示学习"><a href="#提示学习" class="headerlink" title="提示学习"></a>提示学习</h1><p>提示（Prompt）是伴随着输入一起给模型的上下文。<br>cloze prompt：填充答案的位置在句中。<br>prefix prompt：填充答案的位置在句末。</p><h2 id="提示技术"><a href="#提示技术" class="headerlink" title="提示技术"></a>提示技术</h2><h2 id="In-Context-Learning"><a href="#In-Context-Learning" class="headerlink" title="In-Context Learning"></a>In-Context Learning</h2><h2 id="思维链"><a href="#思维链" class="headerlink" title="思维链"></a>思维链</h2><h3 id="链式思维-Chain-of-Thoughts"><a href="#链式思维-Chain-of-Thoughts" class="headerlink" title="链式思维(Chain of Thoughts)"></a>链式思维(Chain of Thoughts)</h3><h3 id="思维树-Tree-of-Thoughts"><a href="#思维树-Tree-of-Thoughts" class="headerlink" title="思维树(Tree of Thoughts)"></a>思维树(Tree of Thoughts)</h3><h3 id="思维图-Graph-of-Thoughts"><a href="#思维图-Graph-of-Thoughts" class="headerlink" title="思维图(Graph of Thoughts)"></a>思维图(Graph of Thoughts)</h3><h2 id="关键步骤"><a href="#关键步骤" class="headerlink" title="关键步骤"></a>关键步骤</h2><h3 id="选择以推理为中心的训练集"><a href="#选择以推理为中心的训练集" class="headerlink" title="选择以推理为中心的训练集"></a>选择以推理为中心的训练集</h3><p>代码本身是Step-by-Step的指令数据。</p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><h1 id="指令微调"><a href="#指令微调" class="headerlink" title="指令微调"></a>指令微调</h1><h1 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>Fine-Tuning Language Models from Human Preferences<br><a href="https://mp.weixin.qq.com/s/TLQ3TdrB5gLb697AFmjEYQ">ChatGPT 背后的“功臣”——RLHF 技术详解</a><br><a href="https://zhuanlan.zhihu.com/p/590655677">ChatGPT发展历程、原理、技术架构详解和产业未来</a><br><a href="https://github.com/chenweiphd/ChatGPT-Hub">https://github.com/chenweiphd/ChatGPT-Hub</a><br><a href="https://www.promptingguide.ai/zh/research/llm-reasoning">大语言模型的推理能力</a><br><a href="https://github.com/SinclairCoder/Instruction-Tuning-Papers">Instruction-Tuning-Papers</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型训练-分布式架构</title>
    <link href="/tech-blog/2025/03/28/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/"/>
    <url>/tech-blog/2025/03/28/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepSpeed"><a href="#DeepSpeed" class="headerlink" title="DeepSpeed"></a>DeepSpeed</h1>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RAG</title>
    <link href="/tech-blog/2025/03/27/%E5%A4%A7%E6%A8%A1%E5%9E%8B-RAG/"/>
    <url>/tech-blog/2025/03/27/%E5%A4%A7%E6%A8%A1%E5%9E%8B-RAG/</url>
    
    <content type="html"><![CDATA[<h1 id="RAG技术概述"><a href="#RAG技术概述" class="headerlink" title="RAG技术概述"></a>RAG技术概述</h1><p>RAG是一种结合了检索机制和大语言模型的技术。它通过从一个长文档或文档集合中检索相关信息，然后将这些信息融合到大模型的生成过程中，以提高模型的回答质量、准确性和实时性。</p><h1 id="特点与优势"><a href="#特点与优势" class="headerlink" title="特点与优势"></a>特点与优势</h1><h2 id="实时性"><a href="#实时性" class="headerlink" title="实时性"></a>实时性</h2><h2 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h2><h2 id="提升特定领域任务的性能"><a href="#提升特定领域任务的性能" class="headerlink" title="提升特定领域任务的性能"></a>提升特定领域任务的性能</h2><p>减轻大模型的幻觉问题。</p><h1 id="技术方案"><a href="#技术方案" class="headerlink" title="技术方案"></a>技术方案</h1><h2 id="检索优化"><a href="#检索优化" class="headerlink" title="检索优化"></a>检索优化</h2><h3 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h3><p>筛选出有利于LM生成质量的内容，过滤掉不相关的检索信息。</p><h2 id="MetaGPT的RAG功能"><a href="#MetaGPT的RAG功能" class="headerlink" title="MetaGPT的RAG功能"></a>MetaGPT的RAG功能</h2><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><h2 id="基于RAG的企业知识库问答"><a href="#基于RAG的企业知识库问答" class="headerlink" title="基于RAG的企业知识库问答"></a>基于RAG的企业知识库问答</h2><h1 id="RAG-vs-Agent"><a href="#RAG-vs-Agent" class="headerlink" title="RAG vs Agent"></a>RAG vs Agent</h1><h2 id="Qwen-Agent-RAG"><a href="#Qwen-Agent-RAG" class="headerlink" title="Qwen-Agent RAG"></a>Qwen-Agent RAG</h2><p>不微调、直接使用上下文为8K的LLM处理百万字上下文。<br>RAG将上下文分割成较短的块，每块不超过512，仅保留最相关的8K在上下文中。</p><h3 id="精准定位最相关的块"><a href="#精准定位最相关的块" class="headerlink" title="精准定位最相关的块"></a>精准定位最相关的块</h3><p>输入构造：将查询中的指令信息和非指令信息分开。<br>推导多语言关键词：从查询的非指令信息推导除多语言关键词。<br>检索：使用BM25检索关键词相关的块。</p><h3 id="相关块与查询词重叠度低"><a href="#相关块与查询词重叠度低" class="headerlink" title="相关块与查询词重叠度低"></a>相关块与查询词重叠度低</h3><p>相关块与查询词重叠度低导致有些相关块无法被检索到。使用类似于搜索中的查询改写方法，将相关块作为查询词，通过BM25检索更多相关块。</p><h3 id="多跳推理"><a href="#多跳推理" class="headerlink" title="多跳推理"></a>多跳推理</h3><p>利用大模型的问题分解、规划和推理能力，将原查询分解成多个问题进行检索，再通过大模型将检索结果整合成回答。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="RAGFlow"><a href="#RAGFlow" class="headerlink" title="RAGFlow"></a>RAGFlow</h2><h2 id="定制化开发"><a href="#定制化开发" class="headerlink" title="定制化开发"></a>定制化开发</h2><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/infiniflow/ragflow">RAGFlow</a><br><a href="https://arxiv.org/pdf/2311.08377.pdf">FILCO：Filter Context for Retrieval-Augmented Generation</a></p><h2 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h2><p><a href="https://qwenlm.github.io/zh/blog/qwen-agent-2405/">qwen-agent</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
      <tag>检索</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型原理</title>
    <link href="/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E4%BB%A3%E8%A1%A8%E6%80%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/"/>
    <url>/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E4%BB%A3%E8%A1%A8%E6%80%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="大模型简介"><a href="#大模型简介" class="headerlink" title="大模型简介"></a>大模型简介</h1><p>NLP任务的发展阶段：（1）基于规则+传统机器学习；（2）预训练语言模型+下游任务微调；（3）提示微调+指令微调；（4）多模态大模型+端到端智能体</p><h2 id="自回归语言模型"><a href="#自回归语言模型" class="headerlink" title="自回归语言模型"></a>自回归语言模型</h2><h2 id="Scaling-Law"><a href="#Scaling-Law" class="headerlink" title="Scaling Law"></a>Scaling Law</h2><h2 id="灾难性遗忘问题"><a href="#灾难性遗忘问题" class="headerlink" title="灾难性遗忘问题"></a>灾难性遗忘问题</h2><h3 id="弹性权重巩固（EWC）"><a href="#弹性权重巩固（EWC）" class="headerlink" title="弹性权重巩固（EWC）"></a>弹性权重巩固（EWC）</h3><p>通过引入正则化项，限制对旧任务重要权重的更新。实现方式：<br>（1）计算Fisher信息矩阵，确定权重对就任务的重要性。<br>（2）在新任务的训练中，添加一个正则化项，限制权重更新幅度。</p><h3 id="经验回放"><a href="#经验回放" class="headerlink" title="经验回放"></a>经验回放</h3><p>保留一部分旧任务的数据，在学习新任务时进行回放，帮助模型在新、旧任务之间保持平衡。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><h2 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h2><h2 id="Transformer-Decoder"><a href="#Transformer-Decoder" class="headerlink" title="Transformer Decoder"></a>Transformer Decoder</h2><p>生成式大语言模型采用的是Transformer Decoder结构。</p><p>LLaMa采用了前置层归一和RMSNorm均方根标准化替代曾标准化。</p><h1 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h1><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>推理能力的大幅增加一部分源于使用代码数据进行训练。</p><h1 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h1><h1 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h1><h2 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h2><h3 id="指令微调"><a href="#指令微调" class="headerlink" title="指令微调"></a>指令微调</h3><h3 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h3><h4 id="强化学习之PPO"><a href="#强化学习之PPO" class="headerlink" title="强化学习之PPO"></a>强化学习之PPO</h4><p>奖励<br>状态<br>行为<br>优化目标</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h1 id="代表性模型"><a href="#代表性模型" class="headerlink" title="代表性模型"></a>代表性模型</h1><h2 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT"></a>GPT</h2><h2 id="LLaMA"><a href="#LLaMA" class="headerlink" title="LLaMA"></a>LLaMA</h2><h3 id="LLaMA3"><a href="#LLaMA3" class="headerlink" title="LLaMA3"></a>LLaMA3</h3><h4 id="模型结构-1"><a href="#模型结构-1" class="headerlink" title="模型结构"></a>模型结构</h4><h4 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h4><h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>RoPE</p><h2 id="-2"><a href="#-2" class="headerlink" title=""></a></h2><h2 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h2><h2 id="DeepSeek"><a href="#DeepSeek" class="headerlink" title="DeepSeek"></a>DeepSeek</h2><h3 id="DeepSeek的影响"><a href="#DeepSeek的影响" class="headerlink" title="DeepSeek的影响"></a>DeepSeek的影响</h3><p>改变了AI大模型商业化的叙事方式，促使OpenAI等大模型的商业模式进行调整。在训练策略方面做出的创新使得训练效果良好的大模型不依赖英伟达的高端显卡成为可能，降低了大模型在训练阶段对高端显卡的依赖，增大了大模型应用的想象空间。应用的爆发会带来大模型推理阶段对显卡的需求。</p><h4 id="模型结构-2"><a href="#模型结构-2" class="headerlink" title="模型结构"></a>模型结构</h4><p>采用了多头潜在注意力(MLA)和DeepSeekMoE架构。</p><h4 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h4><p>负载平衡策略<br>采用了多token预测训练目标，验证对模型性能有益且，并且可用于推理加速。</p><h4 id="语料"><a href="#语料" class="headerlink" title="语料"></a>语料</h4><p>14.8万亿token</p><h3 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h3><h3 id="蒸馏"><a href="#蒸馏" class="headerlink" title="蒸馏"></a>蒸馏</h3><h3 id="后训练"><a href="#后训练" class="headerlink" title="后训练"></a>后训练</h3><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://arxiv.org/pdf/2301.00234">A Survey on In-context Learning</a></p><p><a href="https://github.com/naklecha/llama3-from-scratch/tree/main">llama3 implemented from scratch</a><br><a href="https://github.com/HarleysZhang/llm_note/blob/main/1-transformer_model/llama1-3%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3.md">llama1模型结构详解</a><br><a href="https://arxiv.org/abs/2503.11486">DeepSeek Innovative Techniques</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型位置编码</title>
    <link href="/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
    <url>/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><p>NLP任务（翻译、阅读理解、文本摘要）高度依赖句子顺序，位置编码可以让模型捕获序列的位置信息，提升语言理解能力。</p><p>图像任务（目标检测、图像分割、）</p><h2 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h2><p>Transformer使用三角函数位置编码。<br>BERT使用一个类似于单词嵌入的可训练的参数矩阵作为位置编码（Position Embedding），与Token Embedding和Segement Embedding相加作为输入。</p><h2 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h2><h3 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h3><h4 id="欧拉公式"><a href="#欧拉公式" class="headerlink" title="欧拉公式"></a>欧拉公式</h4>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多模态大模型：技术演进与最新进展</title>
    <link href="/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    <url>/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E6%A8%A1%E6%80%81/</url>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script>  MathJax = {    tex: {      inlineMath: [['$', '$'], ['\\(', '\\)']],      displayMath: [['$$', '$$'], ['\\[', '\\]']]    }  };</script><h1 id="多模态大模型：技术演进与最新进展"><a href="#多模态大模型：技术演进与最新进展" class="headerlink" title="多模态大模型：技术演进与最新进展"></a>多模态大模型：技术演进与最新进展</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>多模态学习（Multimodal Learning）是指AI系统同时处理和理解多种不同类型数据（如文本、图像、音频、视频等）的能力。多模态大模型通过融合不同模态的信息，实现了比单一模态更全面的理解和生成能力，为人工智能带来了质的飞跃。本文将梳理多模态技术的发展历程，探讨关键创新点，并介绍最新研究进展。</p><h2 id="多模态学习的基本概念"><a href="#多模态学习的基本概念" class="headerlink" title="多模态学习的基本概念"></a>多模态学习的基本概念</h2><p>多模态学习的核心挑战在于如何有效融合不同模态的信息。这主要涉及三个关键问题：</p><ol><li><strong>表示学习</strong>：如何将不同模态的数据映射到统一的特征空间</li><li><strong>对齐问题</strong>：如何建立不同模态之间的对应关系</li><li><strong>融合策略</strong>：如何整合多个模态的信息以获得更全面的理解</li></ol><h2 id="技术演进时间线"><a href="#技术演进时间线" class="headerlink" title="技术演进时间线"></a>技术演进时间线</h2><h3 id="早期探索阶段-2010-2015"><a href="#早期探索阶段-2010-2015" class="headerlink" title="早期探索阶段 (2010-2015)"></a>早期探索阶段 (2010-2015)</h3><ul><li><p><strong>2010-2012</strong>: 早期多模态研究主要集中在简单的图像描述任务</p><ul><li><strong>2010</strong>: Farhadi等人提出使用三元组（对象-动作-场景）进行图像描述</li><li><strong>2012</strong>: Socher等人提出将图像与句子映射到共同的多模态空间</li></ul></li><li><p><strong>2013-2014</strong>: 神经网络开始应用于多模态学习</p><ul><li><strong>2013</strong>: DeViSE模型（Frome等人）首次尝试使用神经网络将视觉和语言信息连接起来</li><li><strong>2014</strong>: Kiros等人提出多模态神经语言模型，将图像特征与单词嵌入结合</li></ul></li><li><p><strong>2015</strong>: 基于注意力机制的图像描述生成</p><ul><li>Xu等人提出的”Show, Attend and Tell”模型首次将注意力机制引入图像描述任务</li><li>Vinyals等人的”Show and Tell”模型使用CNN+LSTM架构进行端到端训练</li></ul></li></ul><h3 id="深度融合阶段-2016-2019"><a href="#深度融合阶段-2016-2019" class="headerlink" title="深度融合阶段 (2016-2019)"></a>深度融合阶段 (2016-2019)</h3><ul><li><p><strong>2016</strong>: 多模态问答和视觉对话系统出现</p><ul><li>VQA数据集（Antol等人）的发布推动了视觉问答研究</li><li>双向注意力网络（BiDAF）提出，增强了多模态信息交互</li></ul></li><li><p><strong>2017-2018</strong>: 跨模态预训练与迁移学习</p><ul><li><strong>2017</strong>: ViLBERT首次尝试将BERT扩展到视觉语言任务</li><li><strong>2018</strong>: LXMERT提出，使用Transformer实现视觉与语言的深度融合</li></ul></li><li><p><strong>2019</strong>: 大规模预训练多模态模型</p><ul><li><strong>VL-BERT</strong>: 通过大规模预训练学习视觉-语言表示</li><li><strong>VisualBERT</strong>: 提出视觉和语言的联合嵌入</li></ul></li></ul><h3 id="多模态大模型兴起-2020-2022"><a href="#多模态大模型兴起-2020-2022" class="headerlink" title="多模态大模型兴起 (2020-2022)"></a>多模态大模型兴起 (2020-2022)</h3><ul><li><p><strong>2020</strong>: 统一架构的多模态模型</p><ul><li><strong>UNITER</strong>: 提出统一的图像-文本表示学习框架</li><li><strong>Oscar</strong>: 使用目标标签作为锚点连接视觉和语言模态</li><li><strong>CLIP</strong>: OpenAI发布对比语言-图像预训练模型，开创了大规模对比学习新范式</li></ul></li><li><p><strong>2021</strong>: 生成式多模态模型突破</p><ul><li><strong>DALL-E</strong>: OpenAI推出可从文本生成图像的模型</li><li><strong>ALBEF</strong>: 提出对齐前融合方法，使图像和文本表示更紧密地对齐</li><li><strong>BLIP</strong>: 提出引导式语言-图像预训练，改进了多模态表示质量</li></ul></li><li><p><strong>2022</strong>: 多模态大模型全面发展</p><ul><li><strong>Flamingo</strong>: DeepMind提出的少样本视觉语言模型</li><li><strong>Imagen</strong>: Google发布的文本到图像扩散模型</li><li><strong>Stable Diffusion</strong>: Stability AI发布开源扩散模型，大大推动了多模态生成领域</li><li><strong>BLIP-2</strong>: 提出基于查询Transformer的视觉语言理解架构</li><li><strong>CoCa</strong>: 对比式跨模态学习，同时解决多个视觉语言任务</li></ul></li></ul><h3 id="大型多模态模型时代-2023-至今"><a href="#大型多模态模型时代-2023-至今" class="headerlink" title="大型多模态模型时代 (2023-至今)"></a>大型多模态模型时代 (2023-至今)</h3><ul><li><p><strong>2023H1</strong>: 多模态大模型集中涌现</p><ul><li><strong>GPT-4V</strong>: OpenAI发布支持视觉输入的GPT-4，开创了多模态大语言模型先河</li><li><strong>LLaVA</strong>: 提出低成本高性能的开源多模态大语言模型</li><li><strong>MiniGPT-4</strong>: 将视觉编码器与强大的LLM连接的精简架构</li><li><strong>PaLM-E</strong>: Google将视觉感知能力融入大语言模型</li><li><strong>Kosmos-1</strong>: 微软发布的多模态大语言模型</li></ul></li><li><p><strong>2023H2</strong>: 多模态能力不断扩展</p><ul><li><strong>Gemini</strong>: Google发布的可处理多种模态输入的大模型</li><li><strong>Claude 2.1</strong>: Anthropic发布的支持图像输入的多模态助手</li><li><strong>GPT-4o</strong>: OpenAI发布的实时多模态模型，支持语音输入和输出</li><li><strong>CLIP-BLIP3</strong>: 增强版多模态理解架构，改进跨模态对齐</li><li><strong>LLaVA-1.5</strong>: 性能提升的LLaVA版本，在开源多模态模型中表现突出</li></ul></li><li><p><strong>2024</strong>: 多模态融合与交互的深度演进</p><ul><li><strong>Gemini 1.5 Pro</strong>: Google提出扩展上下文长度的多模态模型（100万token）</li><li><strong>Claude 3</strong>: Anthropic推出具有更强视觉能力的模型系列</li><li><strong>VILA</strong>: 统一的视觉语言架构，改进不同模态信息的交互方式</li><li><strong>Llama 3</strong>: Meta发布的新一代多模态大语言模型，改进跨模态理解</li><li><strong>SEED</strong>: 突破性的视频理解与生成模型，支持长视频理解</li></ul></li></ul><h2 id="关键创新与技术突破"><a href="#关键创新与技术突破" class="headerlink" title="关键创新与技术突破"></a>关键创新与技术突破</h2><h3 id="1-统一表示学习"><a href="#1-统一表示学习" class="headerlink" title="1. 统一表示学习"></a>1. 统一表示学习</h3><p>多模态学习最根本的挑战是如何在统一的表示空间中处理不同模态信息。其关键创新包括：</p><ul><li><strong>跨模态投影</strong>：将不同模态映射到共享语义空间（CLIP, ALIGN）</li><li><strong>对比学习</strong>：使用对比损失拉近匹配样本，推远不匹配样本（CLIP, ALBEF）</li><li><strong>联合编码</strong>：单一通道同时处理多模态输入（ViLBERT, ALBEF）</li></ul><h3 id="2-注意力与跨模态对齐"><a href="#2-注意力与跨模态对齐" class="headerlink" title="2. 注意力与跨模态对齐"></a>2. 注意力与跨模态对齐</h3><p>解决不同模态间对齐问题的关键技术：</p><ul><li><strong>跨模态注意力</strong>：允许一个模态的特征关注另一模态中的相关部分（ViLBERT）</li><li><strong>基于图的对齐</strong>：使用图结构显式建模模态间关系（SGRAF）</li><li><strong>对齐前融合</strong>：在融合前先将不同模态特征对齐（ALBEF, FLAVA）</li></ul><h3 id="3-视觉-语言预训练任务"><a href="#3-视觉-语言预训练任务" class="headerlink" title="3. 视觉-语言预训练任务"></a>3. 视觉-语言预训练任务</h3><p>推动多模态发展的预训练任务设计：</p><ul><li><strong>掩码语言建模</strong>：预测被遮蔽的文本片段（BERT风格）</li><li><strong>掩码图像建模</strong>：重建被遮蔽的图像区域（BEiT, MAE）</li><li><strong>图像-文本匹配</strong>：判断图像和文本是否匹配（CLIP, ALBEF）</li><li><strong>视觉问答</strong>：回答关于图像的问题（VQA, ViLBERT）</li><li><strong>图像描述生成</strong>：为图像生成描述文本（Show and Tell, BLIP）</li></ul><h3 id="4-多模态大语言模型架构"><a href="#4-多模态大语言模型架构" class="headerlink" title="4. 多模态大语言模型架构"></a>4. 多模态大语言模型架构</h3><p>当前主流的多模态大语言模型架构有四种：</p><ul><li><strong>视觉编码器+大语言模型</strong>：使用视觉编码器提取特征，输入到LLM（LLaVA, MiniGPT-4）</li><li><strong>端到端训练</strong>：从头训练包含多模态处理能力的模型（Gemini, GPT-4V）</li><li><strong>模块化设计</strong>：使用专门的模块处理不同模态（BLIP-2, Flamingo）</li><li><strong>统一表示空间</strong>：所有模态映射到同一嵌入空间（CLIP, ALIGN）</li></ul><h3 id="5-知识融合与指令调优"><a href="#5-知识融合与指令调优" class="headerlink" title="5. 知识融合与指令调优"></a>5. 知识融合与指令调优</h3><ul><li><strong>多模态指令微调</strong>：根据多模态指令调整模型行为（LLaVA-Instruct）</li><li><strong>知识蒸馏</strong>：从高性能模型向轻量模型转移能力（MiniGPT-4）</li><li><strong>多阶段训练</strong>：先对齐，再理解，最后生成（BLIP-2, LLaVA）</li></ul><h2 id="最新研究进展"><a href="#最新研究进展" class="headerlink" title="最新研究进展"></a>最新研究进展</h2><h3 id="视觉-语言融合的深化"><a href="#视觉-语言融合的深化" class="headerlink" title="视觉-语言融合的深化"></a>视觉-语言融合的深化</h3><ul><li><strong>视觉语言融合 (VLF)</strong>：更高效的图像-文本交互方法，提高了视觉信息在语言推理中的利用效率</li><li><strong>统一视觉中枢</strong>：统一处理图像、视频和文本的架构，简化了多模态处理流程</li><li><strong>长上下文多模态理解</strong>：扩展多模态处理的上下文窗口，支持更复杂的理解任务</li></ul><h3 id="多模态生成的突破"><a href="#多模态生成的突破" class="headerlink" title="多模态生成的突破"></a>多模态生成的突破</h3><ul><li><strong>一致性增强</strong>：改进模型生成内容与多模态输入的一致性（Imagen 2, DALL-E 3）</li><li><strong>Video-LLM</strong>：支持视频理解和生成的大语言模型架构（VideoChat, Video-LLaMA）</li><li><strong>多模态创意生成</strong>：基于语言描述生成多种形式的内容，包括图像、视频和音频</li></ul><h3 id="高效多模态模型"><a href="#高效多模态模型" class="headerlink" title="高效多模态模型"></a>高效多模态模型</h3><ul><li><strong>参数高效微调</strong>：使用LoRA等技术降低多模态模型训练成本</li><li><strong>量化与压缩</strong>：降低多模态模型的资源需求，实现高效部署</li><li><strong>多模态检索增强</strong>：结合外部知识库增强多模态理解能力</li></ul><h3 id="多模态评估的进展"><a href="#多模态评估的进展" class="headerlink" title="多模态评估的进展"></a>多模态评估的进展</h3><ul><li><strong>多维度评估</strong>：从忠实度、流畅性、相关性等多维度评估多模态模型</li><li><strong>跨模态一致性</strong>：评估不同模态生成内容之间的一致性</li><li><strong>人类对齐评估</strong>：评估多模态模型与人类偏好的一致程度</li></ul><h2 id="应用突破"><a href="#应用突破" class="headerlink" title="应用突破"></a>应用突破</h2><p>多模态大模型已在多个领域实现了重要应用突破：</p><h3 id="内容创作与设计"><a href="#内容创作与设计" class="headerlink" title="内容创作与设计"></a>内容创作与设计</h3><ul><li><strong>AI辅助设计</strong>：基于文本描述生成图像、视频和3D模型</li><li><strong>多模态内容编辑</strong>：跨模态内容编辑和生成，如基于图像或音频的文本生成</li><li><strong>创意协作</strong>：人机协作的创意内容生成流程</li></ul><h3 id="医疗健康"><a href="#医疗健康" class="headerlink" title="医疗健康"></a>医疗健康</h3><ul><li><strong>多模态医疗诊断</strong>：结合医学影像、临床文本和患者数据进行诊断</li><li><strong>辅助医学研究</strong>：分析医学文献和数据，辅助医学研究</li><li><strong>患者交互</strong>：改善医患沟通和医疗服务体验</li></ul><h3 id="教育与学习"><a href="#教育与学习" class="headerlink" title="教育与学习"></a>教育与学习</h3><ul><li><strong>个性化学习体验</strong>：根据学习者需求提供多模态学习内容</li><li><strong>交互式知识探索</strong>：通过多模态对话探索复杂知识</li><li><strong>可视化教学</strong>：将抽象概念可视化，提高理解效率</li></ul><h3 id="工业应用"><a href="#工业应用" class="headerlink" title="工业应用"></a>工业应用</h3><ul><li><strong>质量检测与分析</strong>：结合视觉和文本数据进行产品质量检测</li><li><strong>多模态监控</strong>：监控工业设备运行状态，预测潜在问题</li><li><strong>自动化流程优化</strong>：优化工业生产流程和决策</li></ul><h2 id="未来发展方向"><a href="#未来发展方向" class="headerlink" title="未来发展方向"></a>未来发展方向</h2><h3 id="多模态理解的深化"><a href="#多模态理解的深化" class="headerlink" title="多模态理解的深化"></a>多模态理解的深化</h3><ul><li><strong>更深层次的语义理解</strong>：超越表面关联，理解模态间的深层语义联系</li><li><strong>常识推理增强</strong>：融入更多常识性知识，提升推理能力</li><li><strong>情境理解</strong>：理解多模态内容的社会和文化背景</li></ul><h3 id="模态拓展"><a href="#模态拓展" class="headerlink" title="模态拓展"></a>模态拓展</h3><ul><li><strong>触觉、味觉等感官模态的融入</strong>：拓展到更多人类感官模态</li><li><strong>3D与物理世界理解</strong>：理解物理世界的结构和规律</li><li><strong>多模态交互系统</strong>：自然、流畅的多模态人机交互系统</li></ul><h3 id="多模态伦理与安全"><a href="#多模态伦理与安全" class="headerlink" title="多模态伦理与安全"></a>多模态伦理与安全</h3><ul><li><strong>防止有害内容生成</strong>：减少错误信息和有害内容的生成</li><li><strong>多模态偏见减轻</strong>：识别和减轻模型中的各种偏见</li><li><strong>隐私保护</strong>：在保护用户隐私的前提下处理多模态数据</li></ul><h3 id="语义web与知识图谱"><a href="#语义web与知识图谱" class="headerlink" title="语义web与知识图谱"></a>语义web与知识图谱</h3><ul><li><strong>多模态知识表示</strong>：构建包含多模态信息的知识图谱</li><li><strong>跨模态语义搜索</strong>：基于语义而非关键词的多模态搜索</li><li><strong>多模态知识库</strong>：构建大规模多模态知识库</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>多模态大模型的发展正在改变人工智能的格局，从单一模态的专家系统向全面理解和生成能力的通用智能转变。虽然仍面临诸多挑战，如模态间的语义鸿沟、计算资源需求和伦理问题，但多模态技术无疑代表了AI发展的重要方向。随着技术的不断进步，多模态大模型将进一步融入我们的日常生活和工作，带来更自然、更智能的人机交互体验。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … &amp; Sutskever, I. (2021). Learning transferable visual models from natural language supervision. ICML 2021.</li><li>Li, J., Li, D., Savarese, S., &amp; Hoi, S. (2023). BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ICML 2023.</li><li>Liu, H., Li, C., Wu, Q., &amp; Lee, Y. J. (2023). Visual instruction tuning. NeurIPS 2023.</li><li>OpenAI. (2023). GPT-4 Technical Report.</li><li>Gemini Team. (2023). Gemini: A Family of Highly Capable Multimodal Models.</li><li>Driess, D., et al. (2023). PaLM-E: An embodied multimodal language model. ICML 2023.</li><li>Alayrac, J. B., et al. (2022). Flamingo: a visual language model for few-shot learning. NeurIPS 2022.</li><li>Huang, Z., et al. (2023). LLaVA: Large Language and Vision Assistant. arXiv preprint.</li><li>Chen, X., et al. (2023). InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. NeurIPS 2023.</li><li>Google. (2024). Gemini 1.5: Pushing the Frontiers of Multimodal AI.</li></ol>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型安全</title>
    <link href="/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%AE%89%E5%85%A8/"/>
    <url>/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%AE%89%E5%85%A8/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型推理加速</title>
    <link href="/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F-%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/"/>
    <url>/tech-blog/2025/03/26/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F-%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h1><p>16bit、8bit量化，通常会损失一定的效果。</p><h1 id="模型结构改进"><a href="#模型结构改进" class="headerlink" title="模型结构改进"></a>模型结构改进</h1><h2 id="Muti-Query-Attention"><a href="#Muti-Query-Attention" class="headerlink" title="Muti-Query Attention"></a>Muti-Query Attention</h2><p>小结：训练速度不变，推理速度提升很大。</p><h2 id="Grouped-Query-Attention"><a href="#Grouped-Query-Attention" class="headerlink" title="Grouped Query Attention"></a>Grouped Query Attention</h2><h2 id="FlashAttention"><a href="#FlashAttention" class="headerlink" title="FlashAttention"></a>FlashAttention</h2><p>分配计算、充分利用GPU。</p><h2 id="PageAttention"><a href="#PageAttention" class="headerlink" title="PageAttention"></a>PageAttention</h2><h2 id="内存墙-Memory-Wall"><a href="#内存墙-Memory-Wall" class="headerlink" title="内存墙(Memory Wall)"></a>内存墙(Memory Wall)</h2><h1 id="Dynamic-Batch"><a href="#Dynamic-Batch" class="headerlink" title="Dynamic Batch"></a>Dynamic Batch</h1><h1 id="KVCache"><a href="#KVCache" class="headerlink" title="KVCache"></a>KVCache</h1><p>解码过程中的冗余计算</p><h2 id="KVCache的进一步优化"><a href="#KVCache的进一步优化" class="headerlink" title="KVCache的进一步优化"></a>KVCache的进一步优化</h2><h2 id="deepseek"><a href="#deepseek" class="headerlink" title="deepseek"></a>deepseek</h2><h3 id="一次生成多个token"><a href="#一次生成多个token" class="headerlink" title="一次生成多个token"></a>一次生成多个token</h3><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><p><a href="https://arxiv.org/pdf/1911.02150">Fast Transformer Decoding</a><br><a href="https://arxiv.org/pdf/2305.13245">Grouped Query Attention</a><br><a href="https://arxiv.org/abs/2503.11486">DeepSeek Innovative Techniques</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>窗口外推技术综述</title>
    <link href="/tech-blog/2025/03/26/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E7%AA%97%E5%8F%A3%E5%A4%96%E6%8E%A8%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/"/>
    <url>/tech-blog/2025/03/26/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-%E7%AA%97%E5%8F%A3%E5%A4%96%E6%8E%A8%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="超长上下文"><a href="#超长上下文" class="headerlink" title="超长上下文"></a>超长上下文</h1><h1 id="窗口外推技术"><a href="#窗口外推技术" class="headerlink" title="窗口外推技术"></a>窗口外推技术</h1><h2 id="直接外推"><a href="#直接外推" class="headerlink" title="直接外推"></a>直接外推</h2><p>提前预留一些维度，训练阶段设为0，推理阶段直接改为其他数字。</p><h2 id="插值"><a href="#插值" class="headerlink" title="插值"></a>插值</h2><h3 id="线性内插"><a href="#线性内插" class="headerlink" title="线性内插"></a>线性内插</h3><p>将不在预训练范围的位置除整数后映射到训练范围，适应相对位置（序信息）更重要的场景。通常需要微调以便适应新的映射关系。</p><h3 id="进制转换"><a href="#进制转换" class="headerlink" title="进制转换"></a>进制转换</h3><h2 id="NTK-aware插值"><a href="#NTK-aware插值" class="headerlink" title="NTK-aware插值"></a>NTK-aware插值</h2><h2 id="YaRN"><a href="#YaRN" class="headerlink" title="YaRN"></a>YaRN</h2><p>本质是一种新的RoPE方法。</p><h2 id="S2-Attention"><a href="#S2-Attention" class="headerlink" title="S2-Attention"></a>S2-Attention</h2><h2 id="Reformer"><a href="#Reformer" class="headerlink" title="Reformer"></a>Reformer</h2>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI工具大全：提升效率的必备神器</title>
    <link href="/tech-blog/2025/03/26/AI%E5%B7%A5%E5%85%B7%E5%A4%A7%E5%85%A8/"/>
    <url>/tech-blog/2025/03/26/AI%E5%B7%A5%E5%85%B7%E5%A4%A7%E5%85%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="AI工具大全：提升效率的必备神器"><a href="#AI工具大全：提升效率的必备神器" class="headerlink" title="AI工具大全：提升效率的必备神器"></a>AI工具大全：提升效率的必备神器</h1><p>本文将为大家介绍目前最实用的AI工具，按照不同应用场景进行分类，帮助你在合适的场景选择最适合的工具。</p><h2 id="内容创作类"><a href="#内容创作类" class="headerlink" title="内容创作类"></a>内容创作类</h2><h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h3><p><strong>ChatGPT</strong></p><ul><li><strong>开发商</strong>：OpenAI</li><li><strong>特点</strong>：基于GPT-4等模型，可进行对话式交互，生成高质量文本</li><li><strong>适用场景</strong>：写作辅助、内容创作、问题解答、头脑风暴等</li><li><strong>链接</strong>：<a href="https://chat.openai.com/">https://chat.openai.com</a></li></ul><p><strong>Claude</strong></p><ul><li><strong>开发商</strong>：Anthropic</li><li><strong>特点</strong>：专注于安全和有帮助性，理解复杂指令能力强，处理长文本能力优秀</li><li><strong>适用场景</strong>：长篇内容创作、复杂文档分析、学术研究等</li><li><strong>链接</strong>：<a href="https://claude.ai/">https://claude.ai</a></li></ul><p><strong>Gemini</strong></p><ul><li><strong>开发商</strong>：Google</li><li><strong>特点</strong>：多模态能力，文本理解和生成能力强，与Google工具生态集成</li><li><strong>适用场景</strong>：多种格式内容创作、知识查询、辅助编程等</li><li><strong>链接</strong>：<a href="https://gemini.google.com/">https://gemini.google.com</a></li></ul><h3 id="图像生成"><a href="#图像生成" class="headerlink" title="图像生成"></a>图像生成</h3><p><strong>Reve Image</strong></p><ul><li><strong>开发商</strong>：Reve AI</li><li><strong>特点</strong>：高质量、快速的AI图像生成工具，提供多种风格选择和自定义选项</li><li><strong>适用场景</strong>：插画创作、产品设计概念图、社交媒体内容、艺术创作</li><li><strong>链接</strong>：<a href="https://preview.reve.art/">https://preview.reve.art</a></li></ul><p><strong>Midjourney</strong></p><ul><li><strong>开发商</strong>：Midjourney Inc.</li><li><strong>特点</strong>：生成艺术感极强的图像，风格多样，质量高</li><li><strong>适用场景</strong>：概念艺术、插图设计、创意灵感、社交媒体图像等</li><li><strong>链接</strong>：通过Discord使用 <a href="https://www.midjourney.com/">https://www.midjourney.com</a></li></ul><p><strong>DALL-E 3</strong></p><ul><li><strong>开发商</strong>：OpenAI</li><li><strong>特点</strong>：文本到图像生成，高度精确的细节控制，支持各种艺术风格</li><li><strong>适用场景</strong>：产品设计、插图创作、概念可视化等</li><li><strong>链接</strong>：通过ChatGPT Plus或<a href="https://openai.com/dall-e-3">https://openai.com/dall-e-3</a></li></ul><p><strong>Stable Diffusion</strong></p><ul><li><strong>开发商</strong>：Stability AI</li><li><strong>特点</strong>：开源，可本地部署，自定义能力强，社区活跃</li><li><strong>适用场景</strong>：图像生成、风格迁移、图像编辑、本地化应用等</li><li><strong>链接</strong>：<a href="https://stability.ai/">https://stability.ai</a></li></ul><h3 id="音频生成"><a href="#音频生成" class="headerlink" title="音频生成"></a>音频生成</h3><p><strong>ElevenLabs</strong></p><ul><li><strong>开发商</strong>：ElevenLabs Inc.</li><li><strong>特点</strong>：超真实的AI语音生成，支持多种语言和情感表达</li><li><strong>适用场景</strong>：有声书、播客、视频配音、虚拟助手等</li><li><strong>链接</strong>：<a href="https://elevenlabs.io/">https://elevenlabs.io</a></li></ul><p><strong>Murf AI</strong></p><ul><li><strong>开发商</strong>：Murf AI</li><li><strong>特点</strong>：专业级AI配音，丰富的声音库，适合商业用途</li><li><strong>适用场景</strong>：营销视频、教学内容、演示文稿等</li><li><strong>链接</strong>：<a href="https://murf.ai/">https://murf.ai</a></li></ul><h3 id="视频生成"><a href="#视频生成" class="headerlink" title="视频生成"></a>视频生成</h3><p><strong>Runway Gen-2</strong></p><ul><li><strong>开发商</strong>：Runway AI</li><li><strong>特点</strong>：文本到视频、图像到视频的高质量生成</li><li><strong>适用场景</strong>：短视频创作、视觉效果、创意内容等</li><li><strong>链接</strong>：<a href="https://runwayml.com/">https://runwayml.com</a></li></ul><p><strong>Synthesia</strong></p><ul><li><strong>开发商</strong>：Synthesia</li><li><strong>特点</strong>：AI视频讲解人，可从文本生成真人视频</li><li><strong>适用场景</strong>：培训视频、营销内容、客户沟通等</li><li><strong>链接</strong>：<a href="https://www.synthesia.io/">https://www.synthesia.io</a></li></ul><p><strong>Pika Labs</strong></p><ul><li><strong>开发商</strong>：Pika Labs</li><li><strong>特点</strong>：简单的文本提示即可生成流畅的短视频</li><li><strong>适用场景</strong>：社交媒体内容、短视频创作、动画生成等</li><li><strong>链接</strong>：<a href="https://pika.art/">https://pika.art</a></li></ul><h2 id="工作效率类"><a href="#工作效率类" class="headerlink" title="工作效率类"></a>工作效率类</h2><h3 id="写作辅助"><a href="#写作辅助" class="headerlink" title="写作辅助"></a>写作辅助</h3><p><strong>Notion AI</strong></p><ul><li><strong>开发商</strong>：Notion</li><li><strong>特点</strong>：集成在Notion中的AI助手，可辅助内容创作、总结和编辑</li><li><strong>适用场景</strong>：笔记整理、文档创建、内容总结等</li><li><strong>链接</strong>：<a href="https://www.notion.so/product/ai">https://www.notion.so/product/ai</a></li></ul><p><strong>Grammarly</strong></p><ul><li><strong>开发商</strong>：Grammarly Inc.</li><li><strong>特点</strong>：AI驱动的写作助手，提供语法纠正、风格建议、清晰度改进等</li><li><strong>适用场景</strong>：学术写作、商业邮件、内容创作等</li><li><strong>链接</strong>：<a href="https://www.grammarly.com/">https://www.grammarly.com</a></li></ul><p><strong>Jasper</strong></p><ul><li><strong>开发商</strong>：Jasper AI</li><li><strong>特点</strong>：专为营销内容设计的AI写作助手，提供多种模板和功能</li><li><strong>适用场景</strong>：博客文章、社交媒体内容、广告文案等</li><li><strong>链接</strong>：<a href="https://www.jasper.ai/">https://www.jasper.ai</a></li></ul><h3 id="时间管理"><a href="#时间管理" class="headerlink" title="时间管理"></a>时间管理</h3><p><strong>Motion</strong></p><ul><li><strong>开发商</strong>：Motion</li><li><strong>特点</strong>：AI日程安排，自动优化你的日程和任务</li><li><strong>适用场景</strong>：时间管理、项目计划、会议安排等</li><li><strong>链接</strong>：<a href="https://www.usemotion.com/">https://www.usemotion.com</a></li></ul><p><strong>Reclaim.ai</strong></p><ul><li><strong>开发商</strong>：Reclaim.ai</li><li><strong>特点</strong>：智能日历助手，自动安排任务和会议</li><li><strong>适用场景</strong>：工作计划、时间分配、团队协作等</li><li><strong>链接</strong>：<a href="https://reclaim.ai/">https://reclaim.ai</a></li></ul><h3 id="邮件管理"><a href="#邮件管理" class="headerlink" title="邮件管理"></a>邮件管理</h3><p><strong>Superhuman</strong></p><ul><li><strong>开发商</strong>：Superhuman</li><li><strong>特点</strong>：AI辅助的邮件客户端，提高邮件处理效率</li><li><strong>适用场景</strong>：高邮件量管理、快速响应、邮件整理等</li><li><strong>链接</strong>：<a href="https://superhuman.com/">https://superhuman.com</a></li></ul><p><strong>Lavender</strong></p><ul><li><strong>开发商</strong>：Lavender</li><li><strong>特点</strong>：AI邮件写作助手，优化邮件内容和响应率</li><li><strong>适用场景</strong>：销售邮件、跟进邮件、客户沟通等</li><li><strong>链接</strong>：<a href="https://www.lavender.ai/">https://www.lavender.ai</a></li></ul><h2 id="开发与设计类"><a href="#开发与设计类" class="headerlink" title="开发与设计类"></a>开发与设计类</h2><h3 id="编程辅助"><a href="#编程辅助" class="headerlink" title="编程辅助"></a>编程辅助</h3><p><strong>GitHub Copilot</strong></p><ul><li><strong>开发商</strong>：GitHub &amp; OpenAI</li><li><strong>特点</strong>：AI代码补全和生成，支持多种编程语言和IDE</li><li><strong>适用场景</strong>：软件开发、代码编写、开发学习等</li><li><strong>链接</strong>：<a href="https://github.com/features/copilot">https://github.com/features/copilot</a></li></ul><p><strong>Cursor</strong></p><ul><li><strong>开发商</strong>：Cursor</li><li><strong>特点</strong>：内置AI功能的代码编辑器，支持代码生成、解释和重构</li><li><strong>适用场景</strong>：软件开发、代码理解、编程学习等</li><li><strong>链接</strong>：<a href="https://cursor.sh/">https://cursor.sh</a></li></ul><p><strong>Replit GhostWriter</strong></p><ul><li><strong>开发商</strong>：Replit</li><li><strong>特点</strong>：在线IDE中集成的AI编程助手</li><li><strong>适用场景</strong>：快速原型开发、学习编程、小项目创建等</li><li><strong>链接</strong>：<a href="https://replit.com/ghostwriter">https://replit.com/ghostwriter</a></li></ul><h3 id="设计辅助"><a href="#设计辅助" class="headerlink" title="设计辅助"></a>设计辅助</h3><p><strong>Figma AI</strong></p><ul><li><strong>开发商</strong>：Figma</li><li><strong>特点</strong>：集成在Figma中的AI设计助手，辅助创建和编辑设计</li><li><strong>适用场景</strong>：UI&#x2F;UX设计、原型创建、设计系统等</li><li><strong>链接</strong>：<a href="https://www.figma.com/ai">https://www.figma.com/ai</a></li></ul><p><strong>Adobe Firefly</strong></p><ul><li><strong>开发商</strong>：Adobe</li><li><strong>特点</strong>：Adobe生态中的生成式AI工具，用于创意内容生成</li><li><strong>适用场景</strong>：创意设计、图像生成与编辑、商业创意等</li><li><strong>链接</strong>：<a href="https://www.adobe.com/products/firefly">https://www.adobe.com/products/firefly</a></li></ul><p><strong>Canva Magic Studio</strong></p><ul><li><strong>开发商</strong>：Canva</li><li><strong>特点</strong>：Canva平台中的AI设计工具集，简化设计流程</li><li><strong>适用场景</strong>：快速设计、社交媒体图片、演示文稿等</li><li><strong>链接</strong>：<a href="https://www.canva.com/magic-studio">https://www.canva.com/magic-studio</a></li></ul><h2 id="研究与学习类"><a href="#研究与学习类" class="headerlink" title="研究与学习类"></a>研究与学习类</h2><h3 id="研究辅助"><a href="#研究辅助" class="headerlink" title="研究辅助"></a>研究辅助</h3><p><strong>AMiner</strong></p><ul><li><strong>开发商</strong>：清华大学知识工程实验室</li><li><strong>特点</strong>：学术挖掘系统，汇集全球科研成果，提供论文查找、学者网络分析和学术趋势挖掘</li><li><strong>适用场景</strong>：学术研究、文献调研、学者关系分析、科研趋势追踪</li><li><strong>链接</strong>：<a href="https://www.aminer.cn/">https://www.aminer.cn</a></li></ul><p><strong>Elicit</strong></p><ul><li><strong>开发商</strong>：Ought</li><li><strong>特点</strong>：AI研究助手，帮助查找和分析研究论文</li><li><strong>适用场景</strong>：学术研究、文献综述、研究规划等</li><li><strong>链接</strong>：<a href="https://elicit.org/">https://elicit.org</a></li></ul><p><strong>Connected Papers</strong></p><ul><li><strong>开发商</strong>：Connected Papers</li><li><strong>特点</strong>：视觉化学术论文关系，发现相关研究</li><li><strong>适用场景</strong>：研究领域探索、文献梳理、相关工作查找等</li><li><strong>链接</strong>：<a href="https://www.connectedpapers.com/">https://www.connectedpapers.com</a></li></ul><h3 id="学习工具"><a href="#学习工具" class="headerlink" title="学习工具"></a>学习工具</h3><p><strong>Duolingo Max</strong></p><ul><li><strong>开发商</strong>：Duolingo</li><li><strong>特点</strong>：结合AI的语言学习应用，提供个性化练习和反馈</li><li><strong>适用场景</strong>：语言学习、口语练习、语法提高等</li><li><strong>链接</strong>：<a href="https://www.duolingo.com/">https://www.duolingo.com</a></li></ul><p><strong>Khan Academy Khanmigo</strong></p><ul><li><strong>开发商</strong>：Khan Academy</li><li><strong>特点</strong>：AI辅导助手，提供个性化学习指导和问题解答</li><li><strong>适用场景</strong>：学科学习、家庭作业辅导、考试准备等</li><li><strong>链接</strong>：<a href="https://www.khanacademy.org/khanmigo">https://www.khanacademy.org/khanmigo</a></li></ul><h2 id="数据分析与商业智能"><a href="#数据分析与商业智能" class="headerlink" title="数据分析与商业智能"></a>数据分析与商业智能</h2><h3 id="数据分析工具"><a href="#数据分析工具" class="headerlink" title="数据分析工具"></a>数据分析工具</h3><p><strong>Obviously AI</strong></p><ul><li><strong>开发商</strong>：Obviously AI</li><li><strong>特点</strong>：无代码AI数据分析平台，快速构建预测模型</li><li><strong>适用场景</strong>：业务预测、客户分析、市场研究等</li><li><strong>链接</strong>：<a href="https://www.obviously.ai/">https://www.obviously.ai</a></li></ul><p><strong>Akkio</strong></p><ul><li><strong>开发商</strong>：Akkio</li><li><strong>特点</strong>：简易AI分析平台，无需数据科学背景即可使用</li><li><strong>适用场景</strong>：销售预测、客户流失分析、市场细分等</li><li><strong>链接</strong>：<a href="https://www.akkio.com/">https://www.akkio.com</a></li></ul><h3 id="商业智能"><a href="#商业智能" class="headerlink" title="商业智能"></a>商业智能</h3><p><strong>Viable</strong></p><ul><li><strong>开发商</strong>：Viable</li><li><strong>特点</strong>：AI驱动的客户反馈分析工具</li><li><strong>适用场景</strong>：用户体验优化、产品改进、客户洞察等</li><li><strong>链接</strong>：<a href="https://www.askviable.com/">https://www.askviable.com</a></li></ul><p><strong>Pecan</strong></p><ul><li><strong>开发商</strong>：Pecan AI</li><li><strong>特点</strong>：AI预测分析平台，面向业务用户</li><li><strong>适用场景</strong>：销售预测、用户行为分析、运营优化等</li><li><strong>链接</strong>：<a href="https://www.pecan.ai/">https://www.pecan.ai</a></li></ul><h2 id="资源和进一步阅读"><a href="#资源和进一步阅读" class="headerlink" title="资源和进一步阅读"></a>资源和进一步阅读</h2><ul><li><a href="https://www.futuretools.io/">AI工具评测网站</a></li><li><a href="https://www.producthunt.com/topics/artificial-intelligence">产品Hunt AI工具集合</a></li><li><a href="https://github.com/collections/ai-powered-apps">开源AI项目汇总</a></li></ul><p>你正在使用哪些AI工具？有什么推荐或使用心得？欢迎在评论区分享！ </p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>技术应用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>工具</tag>
      
      <tag>效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI编程与VibeCoding</title>
    <link href="/tech-blog/2025/03/26/AI%E7%BC%96%E7%A8%8BVibeCoding/"/>
    <url>/tech-blog/2025/03/26/AI%E7%BC%96%E7%A8%8BVibeCoding/</url>
    
    <content type="html"><![CDATA[<h3 id="Cursor"><a href="#Cursor" class="headerlink" title="Cursor"></a>Cursor</h3><h4 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h4><p>原则：用户提供一系列包含详细信息的文档，为AI设定明确的上下文边界。</p><p>六类核心文档：</p><h5 id="PRD（项目需求文档）"><a href="#PRD（项目需求文档）" class="headerlink" title="PRD（项目需求文档）"></a>PRD（项目需求文档）</h5><p>让人工智能对您正在构建的内容有了深入的高层理解。</p><p>内容包括：【视频1】<br>• 应用概述<br>• 用户流程说明<br>• 技术栈与API信息<br>• 核心功能<br>• 项目的边界（哪些在范围内，哪些不在）</p><h5 id="应用流程文档"><a href="#应用流程文档" class="headerlink" title="应用流程文档"></a>应用流程文档</h5><p>相当于应用的导航地图。地图越清晰，AI的表现越稳定。  </p><p>关键要点：【视频2】<br>• 描述每一个页面的功能与结构<br>• 用户如何从一个页面跳转到另一个页面<br>• 使用简单直接的语言，避免模糊和抽象<br>• 尽可能具体，帮助AI明确路径，减少混淆</p><h5 id="技术栈文档"><a href="#技术栈文档" class="headerlink" title="技术栈文档"></a>技术栈文档</h5><p>明确告诉AI应使用哪些技术组件。  </p><p>应包含：【视频3】<br>• 所有相关软件包与依赖项的清单<br>• API文档链接（AI可以读取）<br>• 首选的库或工具（如Supabase、Stripe、NextAuth等）</p><p>这样可有效避免AI随意“发挥”，减少使用错误技术的幻觉。</p><h5 id="前端设计指南"><a href="#前端设计指南" class="headerlink" title="前端设计指南"></a>前端设计指南</h5><p>教会AI你的视觉语言，确保UI风格一致。</p><p>包含：【视频4】<br>• 字体<br>• 色彩搭配<br>• 间距与布局规范<br>• 首选的UI库或框架<br>• 图标集</p><h5 id="后端结构文档：对使用Supabase或Firebase尤为关键。"><a href="#后端结构文档：对使用Supabase或Firebase尤为关键。" class="headerlink" title="后端结构文档：对使用Supabase或Firebase尤为关键。"></a>后端结构文档：对使用Supabase或Firebase尤为关键。</h5><p>需涵盖：【视频5】<br>• 数据库结构<br>• 用户认证逻辑<br>• 存储规则<br>• 特殊边界情况处理方式  </p><p>只有清楚这些前提，AI才能正确生成SQL等后端逻辑。</p><h5 id="实施计划：这是抗“幻觉”的关键步骤。【视频6】"><a href="#实施计划：这是抗“幻觉”的关键步骤。【视频6】" class="headerlink" title="实施计划：这是抗“幻觉”的关键步骤。【视频6】"></a>实施计划：这是抗“幻觉”的关键步骤。【视频6】</h5><p>将手动开发时需执行的50多个步骤详列出来，交由Cursor或Windsurf执行。</p><p>如此一来，AI便不再进行猜测，而是直接执行。</p><h3 id="Trae"><a href="#Trae" class="headerlink" title="Trae"></a>Trae</h3><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.bilibili.com/video/BV16GN4e7ERA/?buvid=XY5D2C15CF4199B5ED6EE69F69422AC4BBE2B&from_spmid=default-value&is_story_h5=false&mid=d+MKyVknc5z+4F6jTmu5tw==&plat_id=116&share_from=ugc&share_medium=android&share_plat=android&share_session_id=43a1c249-2941-4674-b28d-eb83bb7cdbcf&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1738896201&unique_k=njG74Bc&up_id=597930426">AI编程分享</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>技术应用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>工具</tag>
      
      <tag>效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Natural Humanoid Walk Using Reinforcement Learning</title>
    <link href="/tech-blog/2025/03/25/Figure/"/>
    <url>/tech-blog/2025/03/25/Figure/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.figure.ai/news/reinforcement-learning-walking">Figure.ai</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NVIDIA Isaac GR00T N1: 通用机器人基础模型</title>
    <link href="/tech-blog/2025/03/25/GR00T-N1/"/>
    <url>/tech-blog/2025/03/25/GR00T-N1/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><h1 id="NVIDIA-Isaac-GR00T-N1"><a href="#NVIDIA-Isaac-GR00T-N1" class="headerlink" title="NVIDIA Isaac GR00T N1"></a>NVIDIA Isaac GR00T N1</h1><p><img src="/tech-blog/model-architecture.png" alt="NVIDIA GR00T-N1机器人"></p><p>NVIDIA推出的通用机器人基础模型GR00T N1具有突破性的能力，能够通过视觉和语言指令控制各种机器人系统。</p><p><a href="https://github.com/NVIDIA/Isaac-GR00T">NVIDIA Isaac GR00T N1 GitHub</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>傅立叶机器人最新研究FreeMotion、ShapeLLM解读</title>
    <link href="/tech-blog/2025/03/25/FreeMotion/"/>
    <url>/tech-blog/2025/03/25/FreeMotion/</url>
    
    <content type="html"><![CDATA[<p><a href="">FreeMotion</a><br><a href="https://github.com/qizekun/ShapeLLM">ShapeLLM</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots解读</title>
    <link href="/tech-blog/2025/03/25/%E8%8B%B1%E4%BC%9F%E8%BE%BEHover/"/>
    <url>/tech-blog/2025/03/25/%E8%8B%B1%E4%BC%9F%E8%BE%BEHover/</url>
    
    <content type="html"><![CDATA[<p><a href="https://hover-versatile-humanoid.github.io/">HOVER主页</a><br><a href="https://arxiv.org/abs/2410.21229">HOVER paper</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型微调</title>
    <link href="/tech-blog/2025/03/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/"/>
    <url>/tech-blog/2025/03/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B-%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script>  MathJax = {    tex: {      inlineMath: [['$', '$'], ['\\(', '\\)']],      displayMath: [['$$', '$$'], ['\\[', '\\]']]    }  };</script><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h1 id="按微调目标划分"><a href="#按微调目标划分" class="headerlink" title="按微调目标划分"></a>按微调目标划分</h1><h2 id="指令微调-Instruction-Tuning"><a href="#指令微调-Instruction-Tuning" class="headerlink" title="指令微调(Instruction Tuning)"></a>指令微调(Instruction Tuning)</h2><p>从模型微调的目标进行划分，可分为提示微调、有监督微调、指令微调等。指令微调可以看作有监督微调（SFT）的一种特殊形式。<br>指令微调的训练目标是提升模型遵循指令的能力。</p><h3 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a>数据集格式</h3><p>一般采用“指令-输入-输出”的格式，常见的数据结构如下：<br>{<br>  “instruction”: “请用简洁的语言解释机器学习的基本概念。”,<br>  “input”: “”,<br>  “output”: “机器学习是一种计算方法，它通过从数据中学习模式，而不是基于规则进行编程。”<br>}</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><h4 id="泛化到未见任务"><a href="#泛化到未见任务" class="headerlink" title="泛化到未见任务"></a>泛化到未见任务</h4><h4 id="在单轮中遵循用户指令"><a href="#在单轮中遵循用户指令" class="headerlink" title="在单轮中遵循用户指令"></a>在单轮中遵循用户指令</h4><h4 id="像人类一样提供帮助"><a href="#像人类一样提供帮助" class="headerlink" title="像人类一样提供帮助"></a>像人类一样提供帮助</h4><h3 id="不同领域指令微调数据集"><a href="#不同领域指令微调数据集" class="headerlink" title="不同领域指令微调数据集"></a>不同领域指令微调数据集</h3><p>大模型在不同领域具有其独特性要求和挑战，比如对话领域需要理解长序列寓意关系，并生成连贯的回复；信息抽取领域需要处理不同规范的信息抽取。针对不同领域的特点需要构建合适的指令微调数据集。</p><h1 id="按微调的参数规模划分"><a href="#按微调的参数规模划分" class="headerlink" title="按微调的参数规模划分"></a>按微调的参数规模划分</h1><p>可划分为全参数微调和参数高效的微调方式。</p><h2 id="参数高效微调-PEFT"><a href="#参数高效微调-PEFT" class="headerlink" title="参数高效微调 (PEFT)"></a>参数高效微调 (PEFT)</h2><p>参数高效微调方法致力于在保持模型性能的同时，显著减少需要训练的参数数量。这类方法特别适合计算资源有限的场景。</p><h3 id="Adapter-Tuning"><a href="#Adapter-Tuning" class="headerlink" title="Adapter Tuning"></a>Adapter Tuning</h3><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>背景：常规的Adapter参数微调方法，增加了额外的训练参数，且改变了模型结构，导致训练时的内存占用增大，推理时长增加。<br>关键假设：针对下游任务微调得到的增量参数矩阵是低秩的。<br>LoRA方法：使用低秩分解近微调的增量参数矩阵。</p><p>$$W_0 + \Delta W &#x3D; W_0 + BA$$</p><p>$W_0$ 为预训练模型参数，$\Delta W$ 为微调参数，使用矩阵的低秩分解 $B$ 和 $A$ 的乘积近似微调参数矩阵，模型微调训练时，更新的参数为 $A$、$B$ 矩阵，在执行推理时，将 $BA$ 与 $W_0$ 进行合并。</p><h4 id="LoRA微调的优点"><a href="#LoRA微调的优点" class="headerlink" title="LoRA微调的优点"></a>LoRA微调的优点</h4><ol><li>一个中心模型可以有多个下游任务</li><li>适配器权重可以与基本模型参数矩阵合并，因此不增加额外的推理计算量，不增加推理延迟。额外增加的计算量只有参数合并的计算量。</li><li>与其他PEFT方法正交，可以多种微调方法组合</li><li>可插拔</li></ol><h4 id="LoRA微调的模型参数选取"><a href="#LoRA微调的模型参数选取" class="headerlink" title="LoRA微调的模型参数选取"></a>LoRA微调的模型参数选取</h4><p>注意力参数、FFN参数。</p><h4 id="LoRA参数初始化"><a href="#LoRA参数初始化" class="headerlink" title="LoRA参数初始化"></a>LoRA参数初始化</h4><p>LoRA参数初始化如何实现和全参数微调一样从预训练权重$W_0$开始？矩阵A初始化为全零矩阵，矩阵B通过高斯核函数进行初始化。不能初始化两个全零矩阵，否则两个权重矩阵的梯度全为0，处于鞍点不能更新参数。</p><h4 id="LoRA的秩如何选择"><a href="#LoRA的秩如何选择" class="headerlink" title="LoRA的秩如何选择"></a>LoRA的秩如何选择</h4><p>秩r作为模型训练的超参，理想期望低秩近似结构$BA$与$\delta W$的表达能力越接近越好。<br>一些经验：（1）下游任务越复杂，所需的r越高；（2）基座模型能力越强，所需的r越小；（3）数据规模越大，所需的r越高。</p><h4 id="LoRA微调相比全参数微调降低了显存占用"><a href="#LoRA微调相比全参数微调降低了显存占用" class="headerlink" title="LoRA微调相比全参数微调降低了显存占用"></a>LoRA微调相比全参数微调降低了显存占用</h4><p>预训练权重$W_0$在LoRA微调中只参与前向计算，但不进行梯度计算和参数更，显存不需要存储$W_0$的梯度和优化器状态。B、A矩阵的秩远低于$W_0$的秩，B、A的权重参数、梯度及优化器状态所占的存储开销远小于$W_0$的梯度和优化器状态的存储开销。</p><h3 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h3><h3 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix Tuning"></a>Prefix Tuning</h3><h3 id="LST"><a href="#LST" class="headerlink" title="LST"></a>LST</h3><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><h3 id="微调大语言模型"><a href="#微调大语言模型" class="headerlink" title="微调大语言模型"></a>微调大语言模型</h3><h4 id="基座模型"><a href="#基座模型" class="headerlink" title="基座模型"></a>基座模型</h4><p>a.基座模型，不太依赖数据格式，理论上效果应该更好<br>b.chat模型，sft需要遵循指令的格式</p><h4 id="垂类语料微调"><a href="#垂类语料微调" class="headerlink" title="垂类语料微调"></a>垂类语料微调</h4><p>a.epoch不要超过1轮，避免灾难性遗忘<br>b.最好加通用语料，通用语料、垂直语料的配比要注意，最好在mini-batch层面都能有通用语料</p><h4 id="继续预训练"><a href="#继续预训练" class="headerlink" title="继续预训练"></a>继续预训练</h4><p>a.知识是在预训练阶段记忆的<br>b.默认语料的储备是非常充足的</p><h4 id="基座模型特点"><a href="#基座模型特点" class="headerlink" title="基座模型特点"></a>基座模型特点</h4><p>LLama<br>a.本身sql的能力比较强<br>b.中文字符太少</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p><a href="https://github.com/tloen/alpaca-lora">alpaca-lora</a><br><a href="https://arxiv.org/abs/2206.06522">Ladder Side-Tuning</a></p><h3 id="审稿机器人"><a href="#审稿机器人" class="headerlink" title="审稿机器人"></a>审稿机器人</h3>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MOE</title>
    <link href="/tech-blog/2025/03/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B-MOE/"/>
    <url>/tech-blog/2025/03/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B-MOE/</url>
    
    <content type="html"><![CDATA[<h1 id="MoE提出"><a href="#MoE提出" class="headerlink" title="MoE提出"></a>MoE提出</h1><h1 id="MoE相关工作"><a href="#MoE相关工作" class="headerlink" title="MoE相关工作"></a>MoE相关工作</h1><h1 id="DeepSeek-MoE"><a href="#DeepSeek-MoE" class="headerlink" title="DeepSeek MoE"></a>DeepSeek MoE</h1><h1 id="MoE设计的核心"><a href="#MoE设计的核心" class="headerlink" title="MoE设计的核心"></a>MoE设计的核心</h1>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLM</tag>
      
      <tag>Research</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>具身智能综述：发展历程与未来展望</title>
    <link href="/tech-blog/2025/03/24/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E7%BB%BC%E8%BF%B0/"/>
    <url>/tech-blog/2025/03/24/%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="具身智能综述：发展历程与未来展望"><a href="#具身智能综述：发展历程与未来展望" class="headerlink" title="具身智能综述：发展历程与未来展望"></a>具身智能综述：发展历程与未来展望</h1><h2 id="具身智能起源"><a href="#具身智能起源" class="headerlink" title="具身智能起源"></a>具身智能起源</h2><p>具身智能(Embodied Intelligence)是指通过物理或虚拟身体与环境互动来学习和适应的智能形式。这一概念最早由认知科学家提出，认为智能不仅仅是抽象的计算过程，更是一种与身体和环境密切相关的适应性能力。在人工智能领域，具身智能的研究旨在创造能够感知环境、做出决策并执行动作的智能系统。</p><h2 id="关键技术与方法"><a href="#关键技术与方法" class="headerlink" title="关键技术与方法"></a>关键技术与方法</h2><p>具身智能研究涉及多个技术领域的融合，包括：</p><ul><li>多模态学习：整合视觉、语言、触觉等多种输入信号</li><li>强化学习：通过尝试与环境互动来学习最优策略</li><li>仿真环境：为智能体提供安全、可控的学习场景</li><li>迁移学习：将模拟环境中学到的知识迁移到真实世界</li></ul><h1 id="一：训练数据来源、动作预测策略、模型训练方法"><a href="#一：训练数据来源、动作预测策略、模型训练方法" class="headerlink" title="一：训练数据来源、动作预测策略、模型训练方法"></a>一：训练数据来源、动作预测策略、模型训练方法</h1><h1 id="VLM和VLA中的动作预测"><a href="#VLM和VLA中的动作预测" class="headerlink" title="VLM和VLA中的动作预测"></a>VLM和VLA中的动作预测</h1><h1 id="端到端及其发展之路"><a href="#端到端及其发展之路" class="headerlink" title="端到端及其发展之路"></a>端到端及其发展之路</h1><h1 id="借鉴大语言模型的发展之路"><a href="#借鉴大语言模型的发展之路" class="headerlink" title="借鉴大语言模型的发展之路"></a>借鉴大语言模型的发展之路</h1><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="http://arxiv.org/abs/2502.15336">Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions</a></li><li>Duan, J., Yu, S., Tan, H. L., Geng, X., Wang, Y., Yang, X., … &amp; Liu, Y. (2024). The Rise and Potential of Large Language Model Based Agents: A Survey.</li><li>Huang, S., Xu, C., Yu, B., &amp; Li, S. (2023). Language models as embodied agents.</li></ul>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>Research</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>具身智能</tag>
      
      <tag>机器人</tag>
      
      <tag>多模态</tag>
      
      <tag>综述</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Agent</title>
    <link href="/tech-blog/2025/03/24/Agent/"/>
    <url>/tech-blog/2025/03/24/Agent/</url>
    
    <content type="html"><![CDATA[<h1 id="Agent发展"><a href="#Agent发展" class="headerlink" title="Agent发展"></a>Agent发展</h1><p>符号式Agents<br>符号智能时代的产物，使用符号抽象规则和推理过程。<br>反馈式Agents<br>强化学习Agents<br>迁移学习、元学习Agents<br>大语言模型Agents</p><h1 id="Agent核心问题"><a href="#Agent核心问题" class="headerlink" title="Agent核心问题"></a>Agent核心问题</h1><h2 id="自主性"><a href="#自主性" class="headerlink" title="自主性"></a>自主性</h2><h2 id="反应性"><a href="#反应性" class="headerlink" title="反应性"></a>反应性</h2><ol><li>额外模态扩展<br>感知空间非常大，通过输出访问给定外部API、具身智能等</li></ol><h2 id="积极主动性"><a href="#积极主动性" class="headerlink" title="积极主动性"></a>积极主动性</h2><p>以任务目标为导向主动采取行动的能力，即对Agent的推理</p><h2 id="社会交互能力"><a href="#社会交互能力" class="headerlink" title="社会交互能力"></a>社会交互能力</h2><p>通过合作、竞争等社交可以更好的完成任务。</p><h1 id="Auto-Agent"><a href="#Auto-Agent" class="headerlink" title="Auto-Agent"></a>Auto-Agent</h1><h1 id="AutoGen"><a href="#AutoGen" class="headerlink" title="AutoGen"></a>AutoGen</h1><h1 id="MetaGPT"><a href="#MetaGPT" class="headerlink" title="MetaGPT"></a>MetaGPT</h1><p>多智能体系统框架。</p><h2 id="MetaGPT提供的RAG功能"><a href="#MetaGPT提供的RAG功能" class="headerlink" title="MetaGPT提供的RAG功能"></a>MetaGPT提供的RAG功能</h2><h1 id="单Agent框架VS多Agent框架"><a href="#单Agent框架VS多Agent框架" class="headerlink" title="单Agent框架VS多Agent框架"></a>单Agent框架VS多Agent框架</h1><p>单Agent框架，主要关注智能体的感知、决策、学习，不涉及多个智能体的交互。</p><h1 id="LLM-Agents"><a href="#LLM-Agents" class="headerlink" title="LLM Agents"></a>LLM Agents</h1><h1 id="Agent框架对比"><a href="#Agent框架对比" class="headerlink" title="Agent框架对比"></a>Agent框架对比</h1><h2 id="Auto-GPT"><a href="#Auto-GPT" class="headerlink" title="Auto-GPT"></a>Auto-GPT</h2><p>规划能力强<br>缺点：接口和变量的定义和Agent框架的概念没有对应，学习成本会高一些。</p><h2 id="Qwen-Agent"><a href="#Qwen-Agent" class="headerlink" title="Qwen-Agent"></a>Qwen-Agent</h2><p>学习成本低<br>入门项目</p><ol><li>调用成本</li></ol><h2 id="Auto-Gen"><a href="#Auto-Gen" class="headerlink" title="Auto-Gen"></a>Auto-Gen</h2><h2 id="主要组件"><a href="#主要组件" class="headerlink" title="主要组件"></a>主要组件</h2><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h2 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h2><h3 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h3><h3 id="规划"><a href="#规划" class="headerlink" title="规划"></a>规划</h3><h3 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h3><h3 id="多智能体协作"><a href="#多智能体协作" class="headerlink" title="多智能体协作"></a>多智能体协作</h3><h2 id="智能体记忆框架"><a href="#智能体记忆框架" class="headerlink" title="智能体记忆框架"></a>智能体记忆框架</h2><h3 id="Graphiti"><a href="#Graphiti" class="headerlink" title="Graphiti"></a>Graphiti</h3><p>Graphiti 构建了具有时间感知的知识图谱，这些图谱会随着时间推移，随着关系和上下文的变化而演变，从而适应智能体的需求。</p><h3 id="Letta"><a href="#Letta" class="headerlink" title="Letta"></a>Letta</h3><p>Letta 是一个开源框架，用于构建具有高级推理能力和透明且长期记忆状态的智能体。Letta 框架是 “白盒” 设计，并且与模型无关。此外，它还允许用户在自己的服务器上以可视化方式测试、调试和观察智能体的行为。</p><h3 id="Mem0"><a href="#Mem0" class="headerlink" title="Mem0"></a>Mem0</h3><p>Mem0 是一个为智能体设计的智能记忆层，能够通过与用户的交互主动学习并适应变化。它结合了大语言模型（LLM）和向量存储技术。LLM 负责从对话中提取关键信息，而向量存储则用于语义搜索和记忆检索。</p><h3 id="Memary"><a href="#Memary" class="headerlink" title="Memary"></a>Memary</h3><p>Memary 赋予 AI 智能体类似人类的记忆能力。它通过知识图谱跟踪实体知识、偏好和聊天历史记录，并且该知识图谱会随着智能体与用户的交互自动更新。</p><h3 id="Cognee"><a href="#Cognee" class="headerlink" title="Cognee"></a>Cognee</h3><p>Cognee 是一个 Python 库，将知识图谱与 RAG 结合起来，为智能体和应用程序构建不断演化的语义记忆。它利用动态知识图谱来维护不同信息之间的关系。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">LLM-Agent-Paper-List</a><br><a href="https://arxiv.org/pdf/2502.05957">AutoAgent</a><br><a href="https://github.com/microsoft/autogen">AutoGen</a><br><a href="https://github.com/geekan/MetaGPT">MetaGPT</a><br><a href="https://github.com/crewAIInc/crewAI">CrewAI</a><br><a href="">Dify</a><br><a href="https://github.com/om-ai-lab/OmAgent">OmAgent</a><br><a href="">LangGraph</a></p><p><a href="https://github.com/plurai-ai/intellagent">intellagent</a><br><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-llm-agents">A Visual Guide to LLM Agents: Exploring the main components of Single- and Multi-Agents</a><br><a href="https://github.com/camel-ai/owl">Optimized Workforce Learning</a></p><p><a href="https://github.com/getzep/graphiti?continueFlag=185e25247d3c66fe408f09ab1870b8ef">graphiti</a><br><a href="https://github.com/mem0ai/mem0?continueFlag=185e25247d3c66fe408f09ab1870b8ef">Mem0</a></p><p><a href="https://github.com/MotiaDev/motia">Motia</a></p><p>###文献</p><ul><li>Duan, J., Yu, S., Tan, H. L., Geng, X., Wang, Y., Yang, X., … &amp; Liu, Y. (2024). The Rise and Potential of Large Language Model Based Agents: A Survey.</li></ul><h3 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h3><p><a href="https://docs.deepwisdom.ai/main/zh/guide/tutorials/agent_101.html">Meta-GPT 智能体入门</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数字分身</title>
    <link href="/tech-blog/2025/03/24/%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/"/>
    <url>/tech-blog/2025/03/24/%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="Second-Me；实现数字版的你"><a href="#Second-Me；实现数字版的你" class="headerlink" title="Second Me；实现数字版的你"></a>Second Me；实现数字版的你</h1><p>基于AI原生记忆、分层记忆建模和Me-Alignment算法，它通过分析理解你的记忆和经历，学习你的思维方式、价值观和行为模式，从而来构建一个高度个性化的AI分身。它可以记住你的重要信息、代表你与他人&#x2F;应用互动，如回复邮件、参与在线讨论等。</p><p>支持本地化部署，数据存储和训练在本地进行</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/mindverse/Second-Me">Second Me</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>注意力机制：深度学习中的关键创新</title>
    <link href="/tech-blog/2025/03/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/tech-blog/2025/03/24/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="注意力机制：深度学习中的关键创新"><a href="#注意力机制：深度学习中的关键创新" class="headerlink" title="注意力机制：深度学习中的关键创新"></a>注意力机制：深度学习中的关键创新</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>注意力机制(Attention Mechanism)是深度学习领域的重要突破，它模拟了人类选择性关注信息的能力，为神经网络赋予了”关注重点”的能力。自2017年提出以来，以注意力机制为核心的Transformer架构已经彻底改变了自然语言处理、计算机视觉等多个领域的发展方向。</p><h2 id="注意力机制的起源"><a href="#注意力机制的起源" class="headerlink" title="注意力机制的起源"></a>注意力机制的起源</h2><p>最早的注意力机制可以追溯到2014年Bahdanau等人在机器翻译任务中提出的方法，被称为”加性注意力”。随后Luong等人提出了”乘性注意力”。而真正的突破点是2017年Google团队在《Attention is All You Need》论文中提出的自注意力(Self-Attention)机制和Transformer架构。</p><h2 id="注意力机制的核心原理"><a href="#注意力机制的核心原理" class="headerlink" title="注意力机制的核心原理"></a>注意力机制的核心原理</h2><h3 id="查询-键-值-Query-Key-Value-模型"><a href="#查询-键-值-Query-Key-Value-模型" class="headerlink" title="查询-键-值(Query-Key-Value)模型"></a>查询-键-值(Query-Key-Value)模型</h3><p>注意力机制的核心是QKV模型：</p><ul><li>查询(Query)：当前位置的信息需求</li><li>键(Key)：所有位置的信息索引</li><li>值(Value)：所有位置的实际信息内容</li></ul><p>通过计算查询与键的相似度，为每个值分配权重，实现信息的选择性关注。</p><h3 id="自注意力计算过程"><a href="#自注意力计算过程" class="headerlink" title="自注意力计算过程"></a>自注意力计算过程</h3><ol><li>线性投影：将输入转换为Q、K、V矩阵</li><li>注意力分数计算：Q与K的点积操作</li><li>缩放与Softmax：归一化得到注意力权重</li><li>加权求和：将权重与V相乘得到输出</li></ol><h2 id="注意力机制的变体"><a href="#注意力机制的变体" class="headerlink" title="注意力机制的变体"></a>注意力机制的变体</h2><ol><li><strong>多头注意力(Multi-head Attention)</strong>：并行运行多组注意力，捕捉不同角度的依赖关系</li><li><strong>掩码注意力(Masked Attention)</strong>：在自回归生成任务中防止信息泄露</li><li><strong>稀疏注意力(Sparse Attention)</strong>：降低计算复杂度，处理长序列</li><li><strong>局部注意力(Local Attention)</strong>：只关注局部窗口内的信息</li></ol><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>注意力机制已经在多个领域取得了突破性成果：</p><ul><li><strong>自然语言处理</strong>：GPT、BERT等大型语言模型</li><li><strong>计算机视觉</strong>：Vision Transformer</li><li><strong>多模态学习</strong>：CLIP、Stable Diffusion</li><li><strong>音频处理</strong>：用于语音识别和生成</li></ul><h2 id="未来发展趋势"><a href="#未来发展趋势" class="headerlink" title="未来发展趋势"></a>未来发展趋势</h2><ol><li>计算效率优化：降低注意力机制的计算复杂度</li><li>长文本建模：突破序列长度限制</li><li>稀疏性与局部性探索：结合CNN的优势</li><li>跨领域融合：注意力机制与其他技术的结合</li></ol><h1 id="DeepSeek"><a href="#DeepSeek" class="headerlink" title="DeepSeek"></a>DeepSeek</h1><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h1 id="FlashAttention"><a href="#FlashAttention" class="headerlink" title="FlashAttention"></a>FlashAttention</h1><h1 id="FlashAttention2"><a href="#FlashAttention2" class="headerlink" title="FlashAttention2"></a>FlashAttention2</h1><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems.</li><li>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. ICLR.</li><li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding.</li></ul>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>NLP</tag>
      
      <tag>深度学习</tag>
      
      <tag>注意力机制</tag>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测</title>
    <link href="/tech-blog/2025/03/24/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <url>/tech-blog/2025/03/24/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="目标检测方法演进"><a href="#目标检测方法演进" class="headerlink" title="目标检测方法演进"></a>目标检测方法演进</h1><h1 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h1><h1 id="目标检测损失函数"><a href="#目标检测损失函数" class="headerlink" title="目标检测损失函数"></a>目标检测损失函数</h1><h2 id="传统损失函数L1-L2-Loss等"><a href="#传统损失函数L1-L2-Loss等" class="headerlink" title="传统损失函数L1&#x2F;L2 Loss等"></a>传统损失函数L1&#x2F;L2 Loss等</h2><p>通常分别优化边界框的中心坐标、宽度、高度。</p><h2 id="IoU-based-Loss"><a href="#IoU-based-Loss" class="headerlink" title="IoU based Loss"></a>IoU based Loss</h2><p>优化预测框与真实框的交并比</p><h3 id="目标检测标注数据"><a href="#目标检测标注数据" class="headerlink" title="目标检测标注数据"></a>目标检测标注数据</h3><h4 id="标注数据"><a href="#标注数据" class="headerlink" title="标注数据"></a>标注数据</h4><p>LabelImg<br>CVAT<br>Labelbox<br>SuperAnnotate</p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>深度学习</category>
      
      <category>计算机视觉</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>深度学习</tag>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搜索中的向量检索：原理、技术与应用</title>
    <link href="/tech-blog/2025/03/24/%E6%90%9C%E7%B4%A2%E4%B8%AD%E7%9A%84%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2/"/>
    <url>/tech-blog/2025/03/24/%E6%90%9C%E7%B4%A2%E4%B8%AD%E7%9A%84%E5%90%91%E9%87%8F%E6%A3%80%E7%B4%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="搜索中的向量检索：原理、技术与应用"><a href="#搜索中的向量检索：原理、技术与应用" class="headerlink" title="搜索中的向量检索：原理、技术与应用"></a>搜索中的向量检索：原理、技术与应用</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>随着深度学习技术的发展，向量检索(Vector Search)在现代搜索系统中扮演着越来越重要的角色。传统的基于关键词的搜索方法难以捕捉语义相似性，而向量检索通过将查询和文档映射到同一向量空间，能够更好地理解用户意图和内容语义。本文将探讨向量检索的基本原理、主要技术以及在搜索领域的应用。</p><h2 id="向量检索基础"><a href="#向量检索基础" class="headerlink" title="向量检索基础"></a>向量检索基础</h2><h3 id="从关键词匹配到语义搜索"><a href="#从关键词匹配到语义搜索" class="headerlink" title="从关键词匹配到语义搜索"></a>从关键词匹配到语义搜索</h3><p>传统搜索主要依赖于布尔检索和TF-IDF等技术，这些方法主要关注词汇的精确匹配。而向量检索通过将文本转换为高维向量，能够捕捉词语之间的语义关系，使得即使使用不同词汇表达的相似概念也能被识别。</p><h3 id="向量表示方法"><a href="#向量表示方法" class="headerlink" title="向量表示方法"></a>向量表示方法</h3><ol><li><strong>词嵌入(Word Embedding)</strong>：如Word2Vec、GloVe</li><li><strong>句子和文档嵌入</strong>：如Doc2Vec、Universal Sentence Encoder</li><li><strong>预训练语言模型</strong>：如BERT、GPT系列生成的向量表示</li><li><strong>多模态嵌入</strong>：结合文本、图像等多种模态的向量表示</li></ol><h2 id="高效向量检索算法"><a href="#高效向量检索算法" class="headerlink" title="高效向量检索算法"></a>高效向量检索算法</h2><p>在大规模数据集上进行向量检索面临计算复杂度问题，以下是几种常用的高效检索算法：</p><h3 id="精确检索算法"><a href="#精确检索算法" class="headerlink" title="精确检索算法"></a>精确检索算法</h3><ul><li><strong>蛮力搜索</strong>：计算查询向量与所有文档向量的相似度</li><li><strong>KD树</strong>：基于空间划分的数据结构</li></ul><h3 id="近似最近邻检索-ANN"><a href="#近似最近邻检索-ANN" class="headerlink" title="近似最近邻检索(ANN)"></a>近似最近邻检索(ANN)</h3><ul><li><strong>局部敏感哈希(LSH)</strong>：将相似向量映射到相同的桶中</li><li><strong>乘积量化(PQ)</strong>：将高维向量分解为低维子向量的笛卡尔积</li><li><strong>层次导航图(HNSW)</strong>：构建多层图结构实现对数级别的搜索复杂度</li><li><strong>向量索引库</strong>：Faiss、Annoy、NMSLIB等开源工具</li></ul><h2 id="向量检索在搜索系统中的应用"><a href="#向量检索在搜索系统中的应用" class="headerlink" title="向量检索在搜索系统中的应用"></a>向量检索在搜索系统中的应用</h2><h3 id="语义搜索"><a href="#语义搜索" class="headerlink" title="语义搜索"></a>语义搜索</h3><p>通过向量表示捕捉查询和文档的语义关系，解决关键词匹配无法处理的同义词、上下文理解等问题。</p><h3 id="多模态搜索"><a href="#多模态搜索" class="headerlink" title="多模态搜索"></a>多模态搜索</h3><p>结合文本、图像、音频等多种模态的向量表示，实现跨模态搜索，如以图搜图、以文搜图等。</p><h3 id="个性化推荐"><a href="#个性化推荐" class="headerlink" title="个性化推荐"></a>个性化推荐</h3><p>结合用户历史行为向量和内容向量，提供个性化的搜索结果和推荐。</p><h3 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h3><p>将问题和可能的答案转化为向量，通过相似度计算找到最匹配的答案。</p><h2 id="工程实践与挑战"><a href="#工程实践与挑战" class="headerlink" title="工程实践与挑战"></a>工程实践与挑战</h2><h3 id="系统架构设计"><a href="#系统架构设计" class="headerlink" title="系统架构设计"></a>系统架构设计</h3><ol><li><strong>在线与离线处理</strong>：预计算文档向量并构建索引</li><li><strong>混合检索策略</strong>：结合关键词匹配与向量检索</li><li><strong>向量索引更新</strong>：处理增量数据的索引更新</li></ol><h3 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h3><ol><li><strong>向量压缩</strong>：降低存储成本和查询延迟</li><li><strong>查询优化</strong>：预过滤、重排序等技术</li><li><strong>分布式部署</strong>：处理大规模向量数据</li></ol><h3 id="评估与调优"><a href="#评估与调优" class="headerlink" title="评估与调优"></a>评估与调优</h3><ol><li><strong>相关性评估</strong>：基于人工标注的评估方法</li><li><strong>向量质量优化</strong>：领域适应、微调等技术</li><li><strong>超参数优化</strong>：索引参数、模型参数等调优</li></ol><h2 id="未来趋势"><a href="#未来趋势" class="headerlink" title="未来趋势"></a>未来趋势</h2><ol><li><strong>更高效的索引算法</strong>：降低内存消耗和提高查询速度</li><li><strong>领域特定的向量表示</strong>：针对垂直领域优化的向量模型</li><li><strong>多模态与交互式搜索</strong>：结合语音、图像等多模态输入</li><li><strong>稀疏与密集混合表示</strong>：结合传统检索与向量检索的优势</li></ol><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><p><a href="https://github.com/facebookresearch/faiss">faiss</a><br><a href="https://github.com/milvus-io/milvus">Milvus</a><br><a href="https://github.com/jina-ai">Jina</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>搜索引擎</category>
      
    </categories>
    
    
    <tags>
      
      <tag>搜索</tag>
      
      <tag>向量检索</tag>
      
      <tag>机器学习</tag>
      
      <tag>信息检索</tag>
      
      <tag>相似度搜索</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>逆运动学：机器人控制中的核心技术</title>
    <link href="/tech-blog/2025/03/24/%E9%80%86%E8%BF%90%E5%8A%A8%E5%AD%A6/"/>
    <url>/tech-blog/2025/03/24/%E9%80%86%E8%BF%90%E5%8A%A8%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script>  MathJax = {    tex: {      inlineMath: [['$', '$'], ['\\(', '\\)']],      displayMath: [['$$', '$$'], ['\\[', '\\]']]    }  };</script><h1 id="逆运动学：机器人控制中的核心技术"><a href="#逆运动学：机器人控制中的核心技术" class="headerlink" title="逆运动学：机器人控制中的核心技术"></a>逆运动学：机器人控制中的核心技术</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>逆运动学(Inverse Kinematics)是机器人控制领域的基础技术，用于解决从末端执行器的位置和姿态反推关节角度的问题。这一技术在机器人操作、动画制作、虚拟现实等领域有着广泛的应用。本文将介绍逆运动学的基本原理、常用算法以及应用案例。</p><h2 id="运动学基础"><a href="#运动学基础" class="headerlink" title="运动学基础"></a>运动学基础</h2><h3 id="正运动学与逆运动学"><a href="#正运动学与逆运动学" class="headerlink" title="正运动学与逆运动学"></a>正运动学与逆运动学</h3><ul><li><strong>正运动学(Forward Kinematics)</strong>：已知各关节角度，计算末端执行器的位置和姿态</li><li><strong>逆运动学(Inverse Kinematics)</strong>：已知末端执行器的位置和姿态，计算可行的关节角度</li></ul><p>从数学角度看，正运动学是一个确定性问题，而逆运动学则是一个多解或无解问题，这使得逆运动学在实际应用中更具挑战性。</p><h3 id="DH参数法"><a href="#DH参数法" class="headerlink" title="DH参数法"></a>DH参数法</h3><p>Denavit-Hartenberg(DH)参数是描述机器人运动学的标准方法，使用四个参数（关节距离、关节角、连杆长度、扭转角）来表示相邻关节之间的空间关系。</p><h2 id="逆运动学解算方法"><a href="#逆运动学解算方法" class="headerlink" title="逆运动学解算方法"></a>逆运动学解算方法</h2><h3 id="解析解法"><a href="#解析解法" class="headerlink" title="解析解法"></a>解析解法</h3><p>对于结构简单的机器人（如6自由度及以下），通常可以推导出解析解。解析解具有计算速度快、精度高的优点，但仅适用于特定结构的机器人。</p><h4 id="代数法"><a href="#代数法" class="headerlink" title="代数法"></a>代数法</h4><p>利用几何关系直接推导关节角度，适用于简单结构。</p><h4 id="几何法"><a href="#几何法" class="headerlink" title="几何法"></a>几何法</h4><p>通过三角函数关系求解，直观但受限于机构复杂度。</p><h3 id="数值解法"><a href="#数值解法" class="headerlink" title="数值解法"></a>数值解法</h3><p>对于复杂结构或冗余自由度的机器人，通常采用数值解法。</p><h4 id="雅可比矩阵法"><a href="#雅可比矩阵法" class="headerlink" title="雅可比矩阵法"></a>雅可比矩阵法</h4><p>利用雅可比矩阵描述关节角度变化与末端执行器位置变化的关系，通过迭代计算求解。</p><ol><li>正向雅可比法：$\Delta\theta &#x3D; J^{-1}(θ)\Delta x$</li><li>伪逆法：处理非方阵雅可比矩阵</li><li>阻尼最小二乘法：提高奇异点附近的稳定性</li></ol><h4 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h4><p>将逆运动学问题转化为优化问题，最小化末端执行器目标位置与当前位置之间的误差。</p><ol><li>梯度下降法</li><li>遗传算法</li><li>粒子群优化</li></ol><h2 id="处理关键挑战"><a href="#处理关键挑战" class="headerlink" title="处理关键挑战"></a>处理关键挑战</h2><h3 id="奇异点处理"><a href="#奇异点处理" class="headerlink" title="奇异点处理"></a>奇异点处理</h3><p>奇异点是机器人某些构型下雅可比矩阵秩亏损的位置，在这些位置附近，小的末端移动可能需要大的关节变化。常用处理方法包括：</p><ol><li>SVD分解</li><li>阻尼因子法</li><li>避障算法</li></ol><h3 id="冗余自由度"><a href="#冗余自由度" class="headerlink" title="冗余自由度"></a>冗余自由度</h3><p>当机器人的自由度多于完成任务所需的自由度时，存在无穷多组解。通过引入次优化目标，可以选择符合特定条件的解：</p><ol><li>关节极限避免</li><li>能量最小化</li><li>障碍物避免</li></ol><h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><h3 id="工业机器人控制"><a href="#工业机器人控制" class="headerlink" title="工业机器人控制"></a>工业机器人控制</h3><p>在自动化生产线上，逆运动学用于精确控制机器人完成焊接、装配、搬运等任务。</p><h3 id="人形机器人与仿生结构"><a href="#人形机器人与仿生结构" class="headerlink" title="人形机器人与仿生结构"></a>人形机器人与仿生结构</h3><p>人形机器人的运动控制更加复杂，需要考虑多个末端执行器（双手、双脚）和稳定性约束。</p><h3 id="计算机动画与虚拟现实"><a href="#计算机动画与虚拟现实" class="headerlink" title="计算机动画与虚拟现实"></a>计算机动画与虚拟现实</h3><p>在动画制作和VR中，逆运动学用于生成角色的自然运动和交互。</p><h3 id="医疗机器人"><a href="#医疗机器人" class="headerlink" title="医疗机器人"></a>医疗机器人</h3><p>在微创手术中，逆运动学用于将医生的手部动作映射到手术机器人的运动。</p><h2 id="最新技术进展"><a href="#最新技术进展" class="headerlink" title="最新技术进展"></a>最新技术进展</h2><h3 id="基于学习的方法"><a href="#基于学习的方法" class="headerlink" title="基于学习的方法"></a>基于学习的方法</h3><ol><li>神经网络逆运动学：使用深度学习直接学习从末端位置到关节角度的映射</li><li>强化学习：通过试错学习最优控制策略</li></ol><h3 id="实时计算优化"><a href="#实时计算优化" class="headerlink" title="实时计算优化"></a>实时计算优化</h3><ol><li>GPU加速计算</li><li>并行算法设计</li><li>近似计算方法</li></ol><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>逆运动学作为机器人控制的核心技术，随着人工智能和计算能力的发展，呈现出与机器学习深度融合的趋势。未来的研发方向包括更高效的算法实现、更鲁棒的奇异点处理，以及针对特定领域的专用求解器开发。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>Craig, J. J. (2009). Introduction to robotics: mechanics and control. Pearson.</li><li>Buss, S. R. (2004). Introduction to inverse kinematics with jacobian transpose, pseudoinverse and damped least squares methods. IEEE Journal of Robotics and Automation.</li><li>Aristidou, A., &amp; Lasenby, J. (2011). FABRIK: A fast, iterative solver for the Inverse Kinematics problem. Graphical Models.</li></ul>]]></content>
    
    
    <categories>
      
      <category>Robotics</category>
      
      <category>控制技术</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Robotics</tag>
      
      <tag>机器人学</tag>
      
      <tag>运动规划</tag>
      
      <tag>控制理论</tag>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>提示词工程：高效指导AI生成内容的艺术</title>
    <link href="/tech-blog/2025/03/24/AI%E5%B7%A5%E5%85%B7Prompts/"/>
    <url>/tech-blog/2025/03/24/AI%E5%B7%A5%E5%85%B7Prompts/</url>
    
    <content type="html"><![CDATA[<style>.prompt-container {  position: relative;  background-color: #f8f9fa;  border: 1px solid #e9ecef;  border-radius: 6px;  padding: 20px;  margin: 20px 0;  font-size: 0.95em;  line-height: 1.6;}.copy-button {  position: absolute;  top: 10px;  right: 10px;  background-color: #007bff;  color: white;  border: none;  border-radius: 4px;  padding: 5px 10px;  font-size: 12px;  cursor: pointer;  transition: background-color 0.2s;}.copy-button:hover {  background-color: #0069d9;}.copy-button.copied {  background-color: #28a745;}</style><h1 id="提示词工程：高效指导AI生成内容的艺术"><a href="#提示词工程：高效指导AI生成内容的艺术" class="headerlink" title="提示词工程：高效指导AI生成内容的艺术"></a>提示词工程：高效指导AI生成内容的艺术</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>随着ChatGPT、Claude、Gemini等大型语言模型的普及，提示词工程（Prompt Engineering）已成为一项重要技能。掌握提示工程相关技能可以更好的了解大语言模型的能力和局限性。本文将分享一些实用的提示词模板，帮助您更好地使用AI工具。</p><h2 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h2><h3 id="思维链"><a href="#思维链" class="headerlink" title="思维链"></a>思维链</h3><h3 id="从简单到复杂-LEAST-TO-MOST"><a href="#从简单到复杂-LEAST-TO-MOST" class="headerlink" title="从简单到复杂(LEAST-TO-MOST)"></a>从简单到复杂(LEAST-TO-MOST)</h3><h3 id="计划与解决-PLAN-AND-SOLVE-PS"><a href="#计划与解决-PLAN-AND-SOLVE-PS" class="headerlink" title="计划与解决(PLAN-AND-SOLVE, PS)"></a>计划与解决(PLAN-AND-SOLVE, PS)</h3><h3 id="对比思维链-对比一致性"><a href="#对比思维链-对比一致性" class="headerlink" title="对比思维链&#x2F;对比一致性"></a>对比思维链&#x2F;对比一致性</h3><h3 id="验证与编辑"><a href="#验证与编辑" class="headerlink" title="验证与编辑"></a>验证与编辑</h3><p>验证与编辑 (Zhao et al. 2023a) 后编辑推理链，获得更符合事实的输出，在多跳推理任务中表现优异。</p><h3 id="推理-行动"><a href="#推理-行动" class="headerlink" title="推理+行动"></a>推理+行动</h3><p>推理+行动 (Yao et al. 2022b) 结合推理和行动，解决多样的推理和决策任务，表现优于基础提示和思维链。</p><h3 id="三跳推理"><a href="#三跳推理" class="headerlink" title="三跳推理"></a>三跳推理</h3><p>三跳推理 (Fei et al. 2023) 模拟人类推理过程，显著超过以前的模型。</p><h3 id="事件链"><a href="#事件链" class="headerlink" title="事件链"></a>事件链</h3><p>事件链 (Bao et al. 2024) 提出摘要任务的提示方法，显著提高摘要任务的得分。</p><h2 id="高级杂志风格卡片生成提示词"><a href="#高级杂志风格卡片生成提示词" class="headerlink" title="高级杂志风格卡片生成提示词"></a>高级杂志风格卡片生成提示词</h2><p>以下是一个用于生成高级时尚杂志风格知识卡片的专业提示词模板。这个提示词特别适合设计师、内容创作者和营销人员使用。来自：<a href="https://mp.weixin.qq.com/s/Cz4Xwsa5ZS6tW-H5YdQzSw?s_trans=5141993101_&s_channel=4">https://mp.weixin.qq.com/s/Cz4Xwsa5ZS6tW-H5YdQzSw?s_trans=5141993101_&amp;s_channel=4</a></p><div class="prompt-container" id="magazine-prompt">  <button class="copy-button" onclick="copyPrompt('magazine-prompt')">复制</button>  <p>你是一位国际顶尖的数字杂志艺术总监和前端开发专家，曾为Vogue、Elle等时尚杂志设计过数字版面，擅长将奢华杂志美学与现代网页设计完美融合，创造出令人惊艳的视觉体验。</p>  <p>请从以下29种设计风格中随机选择1种，设计高级时尚杂志风格的知识卡片，将日常信息以精致奢华的杂志编排呈现，让用户感受到如同翻阅高端杂志般的视觉享受。</p>  <p><strong>可选设计风格：</strong></p>  <p>1. 极简主义风格 (Minimalist)<br>  采用极简主义风格设计，遵循"少即是多"的理念。使用大量留白创造呼吸空间，仅保留最必要的元素。配色方案限制在2-3种中性色，主要为白色背景配以黑色或深灰色文字。排版应精确到像素级别，使用精心设计的网格系统和黄金比例。字体选择无衬线字体如Helvetica或Noto Sans，字重变化作为主要层次手段。装饰元素几乎为零，仅使用极细的分隔线和微妙的阴影。整体设计应呈现出克制、优雅且永恒的美学，让内容本身成为焦点。参考Dieter Rams的设计原则和日本无印良品(MUJI)的产品美学。</p>  <p>2. 大胆现代风格 (Bold Modern)<br>  采用大胆现代风格设计，打破传统排版规则，创造强烈视觉冲击。使用鲜艳对比色如荧光粉、电子蓝、亮黄等，背景可使用深色或鲜艳色块。排版应不对称且动态，标题文字极大（至少60px），可使用极粗字重或压缩字体，甚至允许文字重叠和溢出。图形元素应用几何形状，边缘锐利，可添加不规则裁切效果。层次感通过大小、颜色和位置的极端对比创造。整体设计应充满张力和活力，像一张视觉宣言，参考Wired杂志和Pentagram设计工作室的作品。添加微妙动效如悬停放大或颜色变换，增强现代感。</p>  <p>3. 优雅复古风格 (Elegant Vintage)<br>  采用优雅复古风格设计，重现20世纪初期印刷品的精致美学。使用米色或淡黄色纸张质感背景，配以深棕、暗红等老式印刷色。字体必须使用衬线字体如Baskerville或Noto Serif，标题可使用装饰性字体。排版应对称且庄重，遵循传统书籍设计原则。装饰元素包括精致的花纹边框、古典分隔线和角落装饰，可添加轻微做旧效果如纸张纹理和微妙污点。图像应用复古滤镜处理，呈现褪色照片效果。整体设计应散发出典雅、成熟且历经时间考验的气质，参考The New Yorker和老式法国时尚杂志的设计语言。</p>  <p><strong>每种风格都应包含以下元素，但视觉表现各不相同：</strong></p>  <ul>    <li>日期区域：以各风格特有的方式呈现当前日期</li>    <li>标题和副标题：根据风格调整字体、大小、排版方式</li>    <li>引用区块：设计独特的引用样式，体现风格特点</li>    <li>核心要点列表：以符合风格的方式呈现列表内容</li>    <li>二维码区域：将二维码融入整体设计</li>    <li>编辑笔记/小贴士：设计成符合风格的边栏或注释</li>  </ul>  <p><strong>技术规范：</strong></p>  <ul>    <li>使用HTML5、Font Awesome、Tailwind CSS和必要的JavaScript      <ul>        <li>Font Awesome: <a href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-100-M/font-awesome/6.0.0/css/all.min.css">https://lf6-cdn-tos.bytecdntp.com/cdn/expire-100-M/font-awesome/6.0.0/css/all.min.css</a></li>        <li>Tailwind CSS: <a href="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/tailwindcss/2.2.19/tailwind.min.css">https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/tailwindcss/2.2.19/tailwind.min.css</a></li>        <li>中文字体: <a href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap">https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&display=swap</a></li>      </ul>    </li>    <li>可考虑添加微妙的动效，如页面载入时的淡入效果或微妙的悬停反馈</li>    <li>确保代码简洁高效，注重性能和可维护性</li>    <li>使用CSS变量管理颜色和间距，便于风格统一</li>    <li>对于液态数字形态主义风格，必须添加流体动态效果和渐变过渡</li>    <li>对于超感官极简主义风格，必须精确控制每个像素和微妙的交互反馈</li>    <li>对于新表现主义数据可视化风格，必须将数据以视觉化方式融入设计</li>  </ul>  <p><strong>输出要求：</strong></p>  <ul>    <li>提供一个完整的HTML文件，包含所有设计风格的卡片</li>    <li>确保风格共享相同的内容，但视觉表现完全不同</li>    <li>代码应当优雅且符合最佳实践，CSS应体现出对细节的极致追求</li>    <li>设计的宽度为400px，高度不超过1280px</li>    <li>对主题内容进行抽象提炼，只显示列点或最核心句引用，让人阅读有收获感</li>    <li>永远用中文输出，装饰元素可用法语、英语等其他语言显得有逼格</li>    <li>二维码截图地址：（必须用）：https://pic.readnow.pro/2025/03/791e29affc7772652c01be54b92e8c43.jpg</li>  </ul>  <p>请以国际顶尖杂志艺术总监的眼光和审美标准，创造风格迥异但同样令人惊艳的数字杂志式卡片，让用户感受到"这不是普通的信息卡片，而是一件可收藏的数字艺术品"。</p>  <p>待处理内容：<br>  日期：2025-03-23<br>  主题：[在此输入您的主题]</p></div><h2 id="学术论文写作助手"><a href="#学术论文写作助手" class="headerlink" title="学术论文写作助手"></a>学术论文写作助手</h2><h2 id="商业报告生成器"><a href="#商业报告生成器" class="headerlink" title="商业报告生成器"></a>商业报告生成器</h2><h2 id="创意故事构思工具"><a href="#创意故事构思工具" class="headerlink" title="创意故事构思工具"></a>创意故事构思工具</h2><h2 id="技术文档编写指南"><a href="#技术文档编写指南" class="headerlink" title="技术文档编写指南"></a>技术文档编写指南</h2><script>function copyPrompt(elementId) {  const promptContainer = document.getElementById(elementId);  const promptText = promptContainer.innerText.replace('复制', '').trim();    // 创建一个临时textarea元素来执行复制  const textarea = document.createElement('textarea');  textarea.value = promptText;  textarea.style.position = 'fixed';  // 防止页面滚动  document.body.appendChild(textarea);  textarea.select();    try {    const successful = document.execCommand('copy');    const button = promptContainer.querySelector('.copy-button');    button.textContent = '已复制!';    button.classList.add('copied');        // 3秒后恢复按钮状态    setTimeout(() => {      button.textContent = '复制';      button.classList.remove('copied');    }, 3000);  } catch (err) {    console.error('复制失败:', err);  }    document.body.removeChild(textarea);}</script><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://www.promptingguide.ai/zh/techniques">提示工程指南</a><br><a href="arxiv.org/pdf/2407.12994">提示工程综述</a></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>技术应用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Prompts</tag>
      
      <tag>提示词工程</tag>
      
      <tag>LLM</tag>
      
      <tag>生成式AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文生图技术解析(一)：扩散模型原理与架构</title>
    <link href="/tech-blog/2025/03/24/%E6%96%87%E7%94%9F%E5%9B%BE%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90%E4%B8%80/"/>
    <url>/tech-blog/2025/03/24/%E6%96%87%E7%94%9F%E5%9B%BE%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90%E4%B8%80/</url>
    
    <content type="html"><![CDATA[<h1 id="文生图技术解析-一-：扩散模型原理与架构"><a href="#文生图技术解析-一-：扩散模型原理与架构" class="headerlink" title="文生图技术解析(一)：扩散模型原理与架构"></a>文生图技术解析(一)：扩散模型原理与架构</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>文生图(Text-to-Image)技术是近年来人工智能领域最引人瞩目的突破之一，它能够根据文本描述生成相应的图像，为创意表达、内容创作和视觉设计带来革命性变化。本文作为文生图技术解析系列的第一篇，将重点介绍当前主流文生图技术的核心——扩散模型(Diffusion Models)的基本原理和架构。</p><h2 id="扩散模型的发展历程"><a href="#扩散模型的发展历程" class="headerlink" title="扩散模型的发展历程"></a>扩散模型的发展历程</h2><h3 id="从GAN到扩散模型"><a href="#从GAN到扩散模型" class="headerlink" title="从GAN到扩散模型"></a>从GAN到扩散模型</h3><p>早期的图像生成主要依赖于生成对抗网络(GAN)，如StyleGAN等。但GAN存在训练不稳定、模式崩溃等问题。2020年，Ho等人提出了去噪扩散概率模型(DDPM)，开启了扩散模型在图像生成领域的新纪元。随后，Song等人的基于分数的生成模型(SGM)和Rombach等人的潜在扩散模型(LDM)进一步推动了扩散模型的发展。</p><h3 id="里程碑式产品"><a href="#里程碑式产品" class="headerlink" title="里程碑式产品"></a>里程碑式产品</h3><ul><li><strong>DALL-E&#x2F;DALL-E 2</strong>：OpenAI开发的先驱性文生图系统</li><li><strong>Imagen</strong>：Google开发的高保真文生图模型</li><li><strong>Stable Diffusion</strong>：稳定扩散模型，首个开源且能在消费级硬件上运行的大型文生图模型</li><li><strong>Midjourney</strong>：以艺术审美著称的闭源文生图服务</li></ul><h2 id="扩散模型的基本原理"><a href="#扩散模型的基本原理" class="headerlink" title="扩散模型的基本原理"></a>扩散模型的基本原理</h2><h3 id="扩散过程的数学基础"><a href="#扩散过程的数学基础" class="headerlink" title="扩散过程的数学基础"></a>扩散过程的数学基础</h3><p>扩散模型基于马尔可夫链的数学框架，包含两个核心过程：</p><ol><li><strong>前向过程(扩散过程)</strong>：逐步向图像添加高斯噪声，直到完全破坏图像结构，变为纯噪声</li><li><strong>反向过程(去噪过程)</strong>：学习如何逐步去除噪声，从随机噪声中恢复出有意义的图像</li></ol><p>前向过程可以用以下方程表示：<br>$q(x_t|x_{t-1}) &#x3D; \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I})$</p><p>其中，$\beta_t$是控制每一步噪声添加量的参数。</p><h3 id="DDPM算法流程"><a href="#DDPM算法流程" class="headerlink" title="DDPM算法流程"></a>DDPM算法流程</h3><ol><li><p><strong>训练阶段</strong>：</p><ul><li>对原始图像施加不同程度的噪声</li><li>训练神经网络预测每一步添加的噪声，最小化预测误差</li><li>损失函数通常为均方误差：$L &#x3D; \mathbb{E}<em>{x_0,\epsilon,t}[||\epsilon - \epsilon</em>\theta(x_t, t)||^2]$</li></ul></li><li><p><strong>采样阶段</strong>：</p><ul><li>从标准正态分布采样初始噪声$x_T \sim \mathcal{N}(0, \mathbf{I})$</li><li>逐步应用学习到的去噪过程，通过迭代方式生成最终图像</li><li>采样方程：$x_{t-1} &#x3D; \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}<em>t}}\epsilon</em>\theta(x_t, t)) + \sigma_t z$，其中$z \sim \mathcal{N}(0, \mathbf{I})$</li></ul></li></ol><h2 id="文本条件扩散模型架构"><a href="#文本条件扩散模型架构" class="headerlink" title="文本条件扩散模型架构"></a>文本条件扩散模型架构</h2><h3 id="条件生成机制"><a href="#条件生成机制" class="headerlink" title="条件生成机制"></a>条件生成机制</h3><p>为了实现文本控制图像生成，需要将文本信息融入扩散过程：</p><ol><li><strong>文本编码器</strong>：使用预训练语言模型(如CLIP文本编码器)将文本提示转化为高维语义向量</li><li><strong>条件注入</strong>：通过交叉注意力机制将文本特征注入到扩散模型的U-Net架构中</li><li><strong>时间步编码</strong>：为每个去噪步骤提供时间信息，指导噪声预测过程</li></ol><h3 id="Stable-Diffusion模型解析"><a href="#Stable-Diffusion模型解析" class="headerlink" title="Stable Diffusion模型解析"></a>Stable Diffusion模型解析</h3><p>Stable Diffusion作为目前最具影响力的开源文生图模型，采用了潜在扩散模型(LDM)的架构：</p><ol><li><strong>VAE编码器</strong>：将高分辨率图像压缩到低维潜在空间</li><li><strong>U-Net主干网络</strong>：在潜在空间中执行扩散和去噪过程</li><li><strong>CLIP文本编码器</strong>：处理文本输入，提取语义特征</li><li><strong>VAE解码器</strong>：将生成的潜在表示重建为最终图像</li></ol><p>这种架构显著减少了计算资源需求，使模型能够在消费级硬件上运行。</p><h2 id="采样技术与优化方法"><a href="#采样技术与优化方法" class="headerlink" title="采样技术与优化方法"></a>采样技术与优化方法</h2><h3 id="快速采样算法"><a href="#快速采样算法" class="headerlink" title="快速采样算法"></a>快速采样算法</h3><p>扩散模型生成过程原本需要数百次迭代，但通过优化采样算法可大幅加速：</p><ol><li><strong>DDIM(确定性扩散)</strong>：通过构建非马尔可夫过程减少采样步骤</li><li><strong>DPM-Solver&#x2F;DPM-Solver++</strong>：基于常微分方程(ODE)求解器的高效采样器</li><li><strong>Euler&#x2F;Euler-a采样器</strong>：简单高效的欧拉方法及其自适应变种</li><li><strong>PLMS(伪线性多步方法)</strong>：利用前几步信息加速收敛</li></ol><h3 id="引导技术"><a href="#引导技术" class="headerlink" title="引导技术"></a>引导技术</h3><p>通过引导技术增强文本对生成过程的控制：</p><ol><li><strong>分类器引导</strong>：利用预训练图像分类器引导生成过程</li><li><strong>无分类器引导(CFG)</strong>：同时进行条件和无条件生成，通过调整两者权重控制文本遵循度<ul><li>$\epsilon_\theta^{CFG}(x_t|y) &#x3D; \epsilon_\theta(x_t|\emptyset) + s \cdot (\epsilon_\theta(x_t|y) - \epsilon_\theta(x_t|\emptyset))$</li></ul></li></ol><h2 id="当前挑战与局限性"><a href="#当前挑战与局限性" class="headerlink" title="当前挑战与局限性"></a>当前挑战与局限性</h2><h3 id="技术挑战"><a href="#技术挑战" class="headerlink" title="技术挑战"></a>技术挑战</h3><ol><li><strong>精确文本对齐</strong>：准确理解复杂文本指令仍有困难</li><li><strong>空间关系理解</strong>：复杂位置关系和视角描述的处理不够精确</li><li><strong>推理效率</strong>：尽管有所改进，生成高质量图像仍需要较长时间</li><li><strong>风格一致性</strong>：在保持艺术风格一致性方面仍有提升空间</li></ol><h3 id="社会影响与伦理考量"><a href="#社会影响与伦理考量" class="headerlink" title="社会影响与伦理考量"></a>社会影响与伦理考量</h3><ol><li><strong>内容安全</strong>：模型可能生成有害、偏见或不适当内容</li><li><strong>著作权问题</strong>：训练数据来源和生成内容的版权归属存在争议</li><li><strong>身份伪造</strong>：可能被用于创建未经授权的肖像或虚假内容</li><li><strong>就业影响</strong>：对视觉艺术工作者和创意产业的潜在影响</li></ol><h2 id="下一代扩散模型发展方向"><a href="#下一代扩散模型发展方向" class="headerlink" title="下一代扩散模型发展方向"></a>下一代扩散模型发展方向</h2><h3 id="技术演进"><a href="#技术演进" class="headerlink" title="技术演进"></a>技术演进</h3><ol><li><strong>多模态融合</strong>：与视频、3D和音频生成技术的结合</li><li><strong>高效架构</strong>：更轻量化和计算高效的模型结构</li><li><strong>个性化技术</strong>：低成本适应用户特定风格和需求的方法</li><li><strong>物理约束理解</strong>：改进对现实世界物理规则的遵循</li></ol><h3 id="应用拓展"><a href="#应用拓展" class="headerlink" title="应用拓展"></a>应用拓展</h3><ol><li><strong>辅助创意设计</strong>：概念艺术、产品原型、品牌素材生成</li><li><strong>教育可视化</strong>：复杂概念的直观图像表达</li><li><strong>医学影像</strong>：医学图像合成和诊断辅助</li><li><strong>娱乐与游戏</strong>：游戏资产生成和互动内容创作</li></ol><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>扩散模型为文生图技术提供了强大的基础，但这仅是开始。随着算法优化、计算资源降低和应用场景拓展，文生图技术将继续深刻改变视觉内容创作的方式。在下一篇文章中，我们将深入探讨提示工程（Prompt Engineering）技术，帮助读者掌握引导AI生成所需图像的艺术。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems.</li><li>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. CVPR.</li><li>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., … &amp; Chen, M. (2021). GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv.</li><li>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … &amp; Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. arXiv.</li></ul>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
      <tag>扩散模型</tag>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文生图技术解析二</title>
    <link href="/tech-blog/2025/03/24/%E6%96%87%E7%94%9F%E5%9B%BE%E6%9C%80%E6%96%B0%E7%A0%94%E7%A9%B6/"/>
    <url>/tech-blog/2025/03/24/%E6%96%87%E7%94%9F%E5%9B%BE%E6%9C%80%E6%96%B0%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="文生图技术解析-二-：生成式文生图"><a href="#文生图技术解析-二-：生成式文生图" class="headerlink" title="文生图技术解析(二)：生成式文生图"></a>文生图技术解析(二)：生成式文生图</h1>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>生成模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>生成式AI</tag>
      
      <tag>深度学习</tag>
      
      <tag>扩散模型</tag>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Humanoid Shadowing and Imitation from Humans文章解读</title>
    <link href="/tech-blog/2025/03/23/HumanPlus/"/>
    <url>/tech-blog/2025/03/23/HumanPlus/</url>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h1 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h1><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h1 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h1><p><a href="https://humanoid-ai.github.io/">主页</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LAPA: Latent Action Pretraining from Videos文章解读</title>
    <link href="/tech-blog/2025/03/23/LAPA/"/>
    <url>/tech-blog/2025/03/23/LAPA/</url>
    
    <content type="html"><![CDATA[<h1 id="LAPA"><a href="#LAPA" class="headerlink" title="LAPA"></a>LAPA</h1><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li>项目主页: <a href="https://openvla.github.io/">LAPA</a></li><li>代码仓库: <a href="https://github.com/openvla/openvla">GitHub - OpenVLA</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OpenVLA：开源视觉-语言-动作模型解读</title>
    <link href="/tech-blog/2025/03/23/OpenVLA/"/>
    <url>/tech-blog/2025/03/23/OpenVLA/</url>
    
    <content type="html"><![CDATA[<h1 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h1><h1 id="OpenVLA"><a href="#OpenVLA" class="headerlink" title="OpenVLA"></a>OpenVLA</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>Prismatic-7B VLM</p><h2 id="关键组件"><a href="#关键组件" class="headerlink" title="关键组件"></a>关键组件</h2><h3 id="视觉编码器"><a href="#视觉编码器" class="headerlink" title="视觉编码器"></a>视觉编码器</h3><h3 id="投影器"><a href="#投影器" class="headerlink" title="投影器"></a>投影器</h3><p>视觉特征映射到语言嵌入空间</p><h3 id="LLM骨干"><a href="#LLM骨干" class="headerlink" title="LLM骨干"></a>LLM骨干</h3><h2 id="微调数据集"><a href="#微调数据集" class="headerlink" title="微调数据集"></a>微调数据集</h2><p>Open X-Embodiment：包含 970k 个机器人操作轨迹，涵盖多种机器人形态、任务和场景。</p><h2 id="微调方法：Lora"><a href="#微调方法：Lora" class="headerlink" title="微调方法：Lora"></a>微调方法：Lora</h2><h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><h3 id="量化推理"><a href="#量化推理" class="headerlink" title="量化推理"></a>量化推理</h3><h1 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h1><h2 id="仅支持单图输入"><a href="#仅支持单图输入" class="headerlink" title="仅支持单图输入"></a>仅支持单图输入</h2><h2 id="推理吞吐量有限"><a href="#推理吞吐量有限" class="headerlink" title="推理吞吐量有限"></a>推理吞吐量有限</h2><h2 id="在测试任务的成功率低于90-，可靠性有待提升"><a href="#在测试任务的成功率低于90-，可靠性有待提升" class="headerlink" title="在测试任务的成功率低于90%，可靠性有待提升"></a>在测试任务的成功率低于90%，可靠性有待提升</h2><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>项目主页: <a href="https://openvla.github.io/">OpenVLA</a></li><li>代码仓库: <a href="https://github.com/openvla/openvla">GitHub - OpenVLA</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QuasiSim: Quasi-Physical Simulators for Dexterous Manipulations Transfer</title>
    <link href="/tech-blog/2025/03/23/QuasiSim/"/>
    <url>/tech-blog/2025/03/23/QuasiSim/</url>
    
    <content type="html"><![CDATA[<p><a href="https://meowuu7.github.io/QuasiSim/">QuasiSim: Quasi-Physical Simulators for Dexterous Manipulations Transfer</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PaLM-E解读</title>
    <link href="/tech-blog/2025/03/23/Palm-E/"/>
    <url>/tech-blog/2025/03/23/Palm-E/</url>
    
    <content type="html"><![CDATA[<h1 id="PaLM-E-Model"><a href="#PaLM-E-Model" class="headerlink" title="PaLM-E Model"></a>PaLM-E Model</h1><h2 id="for-robotics"><a href="#for-robotics" class="headerlink" title="for robotics"></a>for robotics</h2><h2 id="generally-capable-vision-and-language-model"><a href="#generally-capable-vision-and-language-model" class="headerlink" title="generally-capable vision-and-language model"></a>generally-capable vision-and-language model</h2><p>可用于视觉任务如：描述图片、目标检测、场景分类，也可用于文本任务如：解数学题、生成代码等。</p><p><a href="https://research.google/blog/palm-e-an-embodied-multimodal-language-model/">PaLM-E</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances解读</title>
    <link href="/tech-blog/2025/03/23/SayCan/"/>
    <url>/tech-blog/2025/03/23/SayCan/</url>
    
    <content type="html"><![CDATA[<h1 id="Do-As-I-Can-Not-As-I-Say-Grounding-Language-in-Robotic-Affordances"><a href="#Do-As-I-Can-Not-As-I-Say-Grounding-Language-in-Robotic-Affordances" class="headerlink" title="Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"></a>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</h1><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://say-can.github.io/">项目主页</a></li><li><a href="https://github.com/google-research/google-research/blob/master/saycan/SayCan-Robot-Pick-Place.ipynb">代码示例</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>UMI In-The-Wild Robot Teaching Without In-The-Wild Robots文章解读</title>
    <link href="/tech-blog/2025/03/23/UMI/"/>
    <url>/tech-blog/2025/03/23/UMI/</url>
    
    <content type="html"><![CDATA[<h1 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h1><h1 id="策略学习"><a href="#策略学习" class="headerlink" title="策略学习"></a>策略学习</h1><p><a href="https://umi-gripper.github.io/">主页</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>NLP</tag>
      
      <tag>ComputerVision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RDT-1B：双手操作的扩散基础模型解读</title>
    <link href="/tech-blog/2025/03/22/RDT%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <url>/tech-blog/2025/03/22/RDT%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="RDT-1B-A-DIFFUSION-FOUNDATION-MODEL-FOR-BIMANUAL-MANIPULATION"><a href="#RDT-1B-A-DIFFUSION-FOUNDATION-MODEL-FOR-BIMANUAL-MANIPULATION" class="headerlink" title="RDT-1B: A DIFFUSION FOUNDATION MODEL FOR BIMANUAL MANIPULATION"></a>RDT-1B: A DIFFUSION FOUNDATION MODEL FOR BIMANUAL MANIPULATION</h1><h1 id=""><a href="#" class="headerlink" title=""></a></h1><p><a href="https://arxiv.org/pdf/2410.07864">RDT-1B: A Diffusion Foundation Model for Bimanual Manipulation</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Research</tag>
      
      <tag>Robotics</tag>
      
      <tag>ComputerVision</tag>
      
      <tag>DiffusionModels</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何找到研究切入点：从文献到创新</title>
    <link href="/tech-blog/2025/03/22/%E5%A6%82%E4%BD%95%E6%89%BE%E7%A0%94%E7%A9%B6%E7%82%B9/"/>
    <url>/tech-blog/2025/03/22/%E5%A6%82%E4%BD%95%E6%89%BE%E7%A0%94%E7%A9%B6%E7%82%B9/</url>
    
    <content type="html"><![CDATA[<h1 id="如何切入研究点"><a href="#如何切入研究点" class="headerlink" title="如何切入研究点"></a>如何切入研究点</h1><h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Academic</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Research</tag>
      
      <tag>AcademicSkills</tag>
      
      <tag>ResearchMethodology</tag>
      
      <tag>Innovation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何高效阅读和理解学术论文</title>
    <link href="/tech-blog/2025/03/22/%E5%A6%82%E4%BD%95%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    <url>/tech-blog/2025/03/22/%E5%A6%82%E4%BD%95%E8%AF%BB%E8%AE%BA%E6%96%87/</url>
    
    <content type="html"><![CDATA[<h1 id="如何高效阅读和理解学术论文"><a href="#如何高效阅读和理解学术论文" class="headerlink" title="如何高效阅读和理解学术论文"></a>如何高效阅读和理解学术论文</h1><h1 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h1><p><a href="https://www.aminer.cn/">AMiner</a><br><a href="https://elicit.org/">https://elicit.org</a><br><a href="https://www.connectedpapers.com/">https://www.connectedpapers.com</a><br><a href="https://paperfinder.allen.ai/chat?continueFlag=ed3c84007e3d48511aa8dfd421fe14c5">Ai2 Paper Finder</a></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/jina-ai/node-DeepResearch">Jina-DeepResearch</a></p>]]></content>
    
    
    <categories>
      
      <category>Research</category>
      
      <category>Academic</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Research</tag>
      
      <tag>AcademicSkills</tag>
      
      <tag>PaperReading</tag>
      
      <tag>Methodology</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Yell at your robot论文解读与复现</title>
    <link href="/tech-blog/2025/03/22/YellAtYourRobot/"/>
    <url>/tech-blog/2025/03/22/YellAtYourRobot/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>DeepLearning</tag>
      
      <tag>Robotics</tag>
      
      <tag>Robot</tag>
      
      <tag>Grasping</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>灵巧手：UniDexGrasp论文解读</title>
    <link href="/tech-blog/2025/03/22/UniDexGrasp/"/>
    <url>/tech-blog/2025/03/22/UniDexGrasp/</url>
    
    <content type="html"><![CDATA[<h1 id="UniDexGrasp-统一框架下的机器人灵巧抓取"><a href="#UniDexGrasp-统一框架下的机器人灵巧抓取" class="headerlink" title="UniDexGrasp: 统一框架下的机器人灵巧抓取"></a>UniDexGrasp: 统一框架下的机器人灵巧抓取</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>机器人灵巧抓取是机器人学和人工智能领域的重要研究方向。本文将详细解读UniDexGrasp论文，这是一个面向多样化物体的统一灵巧抓取框架。UniDexGrasp通过结合视觉感知、触觉反馈和强化学习，实现了对未知物体的鲁棒抓取能力，大幅提升了机器人在复杂环境中的操作能力。</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>二指夹爪的局限性：</p><p>目标：学习一个通用的灵巧手抓取方法，在仿真环境泛化到数百中见过或未见过的</p><p>平行夹持器7个自由度，而ShadowHand有26个自由度。高维度加大了生成有效抓取姿势和规划执行轨迹的难度。提出两阶段，评估结果体现出方法：高抓取质量、高多样性的优势。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="生成有效抓取手势"><a href="#生成有效抓取手势" class="headerlink" title="生成有效抓取手势"></a>生成有效抓取手势</h3><p>输入：物体点云输入，输出：若干抓取姿势<br>抓取姿势的表征：</p><p>生成抓取手势共分为3个子模块：（1）生成总体旋转的模块 GraspIPDF，（2），（3）</p><h4 id="GraspIPDF"><a href="#GraspIPDF" class="headerlink" title="GraspIPDF"></a>GraspIPDF</h4><h4 id="GraspGlow"><a href="#GraspGlow" class="headerlink" title="GraspGlow"></a>GraspGlow</h4><p>自监督损失函数：1. 预测的理想接触图与由 GraspGlow 输出的手计算得到的接触图之前的差异；2. 物体点云穿透进手的网格的距离平方值；3. 手上预先选定的点位穿透进平面的深度；4. 自穿透。</p><h4 id="ContactNet"><a href="#ContactNet" class="headerlink" title="ContactNet"></a>ContactNet</h4><h3 id="规划执行轨迹"><a href="#规划执行轨迹" class="headerlink" title="规划执行轨迹"></a>规划执行轨迹</h3><h2 id="抓取策略训练3技巧"><a href="#抓取策略训练3技巧" class="headerlink" title="抓取策略训练3技巧"></a>抓取策略训练3技巧</h2><h3 id="状态规范化"><a href="#状态规范化" class="headerlink" title="状态规范化"></a>状态规范化</h3><h3 id="物体课程学习"><a href="#物体课程学习" class="headerlink" title="物体课程学习"></a>物体课程学习</h3><h3 id="使用分类任务协助训练"><a href="#使用分类任务协助训练" class="headerlink" title="使用分类任务协助训练"></a>使用分类任务协助训练</h3><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><h3 id="机器人灵巧手抓取数据集"><a href="#机器人灵巧手抓取数据集" class="headerlink" title="机器人灵巧手抓取数据集"></a>机器人灵巧手抓取数据集</h3><p>133个物体类别、5519个物体示例、100多万种抓取姿势。</p><h2 id="核心技术"><a href="#核心技术" class="headerlink" title="核心技术"></a>核心技术</h2><h3 id="1-视觉-触觉融合感知"><a href="#1-视觉-触觉融合感知" class="headerlink" title="1. 视觉-触觉融合感知"></a>1. 视觉-触觉融合感知</h3><p>UniDexGrasp采用了多模态感知系统，包括：</p><ul><li><strong>RGB-D相机</strong>：捕获物体的几何形状和外观特征</li><li><strong>触觉传感器</strong>：获取接触力和滑动信息</li><li><strong>自监督特征提取</strong>：无需人工标注的特征学习</li></ul><p>这种融合方式使机器人能够像人类一样，同时利用视觉和触觉信息指导抓取动作。</p><h3 id="2-层次化强化学习"><a href="#2-层次化强化学习" class="headerlink" title="2. 层次化强化学习"></a>2. 层次化强化学习</h3><p>框架采用了层次化的强化学习结构：</p><ul><li><strong>高层策略</strong>：决定整体抓取姿态和方法</li><li><strong>中层策略</strong>：控制手指运动顺序和协调</li><li><strong>低层控制器</strong>：精确控制关节力矩和位置</li></ul><p>这种分层设计大大降低了学习难度，加速了训练过程。</p><h3 id="3-模拟到现实迁移"><a href="#3-模拟到现实迁移" class="headerlink" title="3. 模拟到现实迁移"></a>3. 模拟到现实迁移</h3><p>为解决sim2real问题，UniDexGrasp采用了：</p><ul><li><strong>域随机化</strong>：在模拟中随机化物理参数和视觉特征</li><li><strong>渐进式学习</strong>：从简单任务到复杂任务的课程学习</li><li><strong>现实世界微调</strong>：通过少量真实世界样本进行适应</li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>论文在多个基准测试和真实机器人上进行了评估：</p><ul><li>在YCB物体集上达到了92%的抓取成功率</li><li>对未见过的物体达到了85%的泛化成功率</li><li>在不同光照和杂乱环境中展现了鲁棒性</li></ul><h2 id="与现有方法的比较"><a href="#与现有方法的比较" class="headerlink" title="与现有方法的比较"></a>与现有方法的比较</h2><p>与现有的方法相比，UniDexGrasp在以下方面显示出优势：</p><table><thead><tr><th>方法</th><th>成功率</th><th>泛化能力</th><th>计算效率</th></tr></thead><tbody><tr><td>DexNet</td><td>85%</td><td>中等</td><td>高</td></tr><tr><td>DexPilot</td><td>88%</td><td>高</td><td>低</td></tr><tr><td>UniDexGrasp</td><td>92%</td><td>高</td><td>中等</td></tr></tbody></table><h2 id="局限性与未来工作"><a href="#局限性与未来工作" class="headerlink" title="局限性与未来工作"></a>局限性与未来工作</h2><p>尽管UniDexGrasp取得了显著成果，但仍存在一些局限性：</p><ol><li>对极细或极软物体的处理能力有限</li><li>实时性在复杂场景中仍有提升空间</li><li>多物体交互场景下的表现需要改进</li></ol><p>未来工作将聚焦于：</p><ul><li>集成语言模型指导复杂操作</li><li>增强物理推理能力</li><li>改进在低资源环境中的适应性</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>UniDexGrasp代表了机器人灵巧抓取领域的重要进展，为通用机器人操纵提供了有效解决方案。通过统一的框架整合多模态感知和层次化学习，该方法展现了强大的性能和泛化能力。随着技术的进一步发展，我们有望看到更加智能和灵活的机器人系统在工业和家庭环境中的广泛应用。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li>Andrychowicz, M., et al. (2020). Learning dexterous in-hand manipulation. The International Journal of Robotics Research.</li><li>Levine, S., et al. (2018). Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International Journal of Robotics Research.</li><li>OpenAI, et al. (2019). Solving Rubik’s Cube with a Robot Hand. arXiv preprint arXiv:1910.07113.</li><li>Kalashnikov, D., et al. (2018). Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. Conference on Robot Learning.</li><li>Pinto, L., &amp; Gupta, A. (2016). Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. IEEE International Conference on Robotics and Automation.</li></ol>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
      <category>AI</category>
      
      <category>Robotics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>DeepLearning</tag>
      
      <tag>Robotics</tag>
      
      <tag>Robot</tag>
      
      <tag>Grasping</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAN神经网络</title>
    <link href="/tech-blog/2025/03/14/GAN/"/>
    <url>/tech-blog/2025/03/14/GAN/</url>
    
    <content type="html"><![CDATA[<h1 id="生成对抗网络（GAN）简介"><a href="#生成对抗网络（GAN）简介" class="headerlink" title="生成对抗网络（GAN）简介"></a>生成对抗网络（GAN）简介</h1><p>生成对抗网络（Generative Adversarial Networks，简称GAN）是一种深度学习模型，由Ian Goodfellow和他的同事们于2014年提出。GAN由两个神经网络组成：生成器（Generator）和判别器（Discriminator），这两个网络相互对抗，通过博弈过程来提高彼此的能力。</p><p><img src="https://i.imgur.com/XVKRM4F.png" alt="GAN Architecture"></p><h2 id="GAN的工作原理"><a href="#GAN的工作原理" class="headerlink" title="GAN的工作原理"></a>GAN的工作原理</h2><p>GAN的工作原理可以类比为一个伪造者和一个鉴定专家之间的博弈：</p><ol><li><strong>生成器（伪造者）</strong>：尝试创建看起来真实的数据（如图像）</li><li><strong>判别器（鉴定专家）</strong>：尝试区分真实数据和生成器创建的假数据</li></ol><p>这两个网络在训练过程中相互竞争：</p><ul><li>生成器试图欺骗判别器，创建越来越逼真的假数据</li><li>判别器试图变得更加精明，更好地区分真假数据</li></ul><p>随着训练的进行，两个网络都会不断改进，最终生成器能够创建非常逼真的数据，而判别器难以区分真假。</p><h2 id="GAN的数学表达"><a href="#GAN的数学表达" class="headerlink" title="GAN的数学表达"></a>GAN的数学表达</h2><p>从数学角度看，GAN的目标函数可以表示为一个极小极大博弈（minimax game）：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">min_G max_D <span class="hljs-built_in">V</span>(D, G) = E_&#123;<span class="hljs-attribute">x</span>~<span class="hljs-built_in">p_data</span>(x)&#125;[log <span class="hljs-built_in">D</span>(x)] + E_&#123;z~<span class="hljs-built_in">p_z</span>(z)&#125;[<span class="hljs-built_in">log</span>(<span class="hljs-number">1</span> - <span class="hljs-built_in">D</span>(<span class="hljs-built_in">G</span>(z)))]<br></code></pre></td></tr></table></figure><p>其中：</p><ul><li>G是生成器网络</li><li>D是判别器网络</li><li>p_data是真实数据分布</li><li>p_z是输入噪声的分布</li><li>D(x)表示判别器认为x是真实数据的概率</li><li>G(z)表示生成器从噪声z生成的数据</li></ul><h2 id="GAN的主要类型"><a href="#GAN的主要类型" class="headerlink" title="GAN的主要类型"></a>GAN的主要类型</h2><p>自2014年以来，GAN已经发展出许多变体，以下是一些最重要的类型：</p><h3 id="1-DCGAN（Deep-Convolutional-GAN）"><a href="#1-DCGAN（Deep-Convolutional-GAN）" class="headerlink" title="1. DCGAN（Deep Convolutional GAN）"></a>1. DCGAN（Deep Convolutional GAN）</h3><p>DCGAN在GAN的基础上使用了卷积神经网络，使其更适合处理图像数据。它引入了一些架构指南，如使用批量归一化、去除全连接层等，大大提高了GAN训练的稳定性。</p><h3 id="2-CGAN（Conditional-GAN）"><a href="#2-CGAN（Conditional-GAN）" class="headerlink" title="2. CGAN（Conditional GAN）"></a>2. CGAN（Conditional GAN）</h3><p>条件GAN通过向生成器和判别器提供额外的条件信息（如类别标签），使模型能够生成特定类别的数据。这使得我们可以控制生成过程，例如生成特定数字的手写体。</p><h3 id="3-CycleGAN"><a href="#3-CycleGAN" class="headerlink" title="3. CycleGAN"></a>3. CycleGAN</h3><p>CycleGAN能够在没有成对训练数据的情况下，学习将图像从一个域转换到另一个域，例如将马变成斑马、夏天变成冬天等。它通过引入循环一致性损失（cycle consistency loss）来实现这一点。</p><h3 id="4-StyleGAN"><a href="#4-StyleGAN" class="headerlink" title="4. StyleGAN"></a>4. StyleGAN</h3><p>StyleGAN引入了一种新的生成器架构，能够在不同的分辨率级别上控制生成图像的风格。它能够生成极其逼真的人脸图像，并允许对不同的面部特征进行精细控制。</p><h2 id="GAN的应用"><a href="#GAN的应用" class="headerlink" title="GAN的应用"></a>GAN的应用</h2><p>GAN已经在多个领域展现出巨大的应用潜力：</p><h3 id="图像生成与编辑"><a href="#图像生成与编辑" class="headerlink" title="图像生成与编辑"></a>图像生成与编辑</h3><ul><li>生成高分辨率、逼真的人脸图像</li><li>图像到图像的转换（如素描转照片）</li><li>图像修复与超分辨率重建</li><li>风格迁移</li></ul><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>GAN可以生成额外的训练数据，帮助解决数据稀缺问题，特别是在医学影像等领域。</p><h3 id="药物发现"><a href="#药物发现" class="headerlink" title="药物发现"></a>药物发现</h3><p>GAN可以用于生成新的分子结构，加速药物发现过程。</p><h3 id="视频生成"><a href="#视频生成" class="headerlink" title="视频生成"></a>视频生成</h3><p>最新的GAN模型能够生成短视频片段，未来可能彻底改变影视制作流程。</p><h2 id="GAN的挑战"><a href="#GAN的挑战" class="headerlink" title="GAN的挑战"></a>GAN的挑战</h2><p>尽管GAN非常强大，但它们也面临一些挑战：</p><ol><li><strong>训练不稳定</strong>：GAN的训练过程可能不稳定，容易出现模式崩溃（mode collapse）等问题</li><li><strong>评估困难</strong>：很难客观地评估GAN的性能</li><li><strong>计算资源需求高</strong>：训练高质量的GAN通常需要大量的计算资源</li><li><strong>伦理问题</strong>：GAN可能被用于生成深度伪造（deepfake）内容，引发隐私和信息真实性问题</li></ol><h2 id="实现一个简单的GAN"><a href="#实现一个简单的GAN" class="headerlink" title="实现一个简单的GAN"></a>实现一个简单的GAN</h2><p>以下是使用PyTorch实现一个简单GAN的代码示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torchvision.datasets <span class="hljs-keyword">import</span> MNIST<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 设置随机种子，确保结果可复现</span><br>torch.manual_seed(<span class="hljs-number">42</span>)<br><br><span class="hljs-comment"># 设备配置</span><br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span>)<br><br><span class="hljs-comment"># 超参数</span><br>batch_size = <span class="hljs-number">64</span><br>z_dimension = <span class="hljs-number">100</span><br>learning_rate = <span class="hljs-number">0.0002</span><br>num_epochs = <span class="hljs-number">50</span><br><br><span class="hljs-comment"># 数据加载和预处理</span><br>transform = transforms.Compose([<br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.5</span>,), (<span class="hljs-number">0.5</span>,))<br>])<br><br>mnist_dataset = MNIST(root=<span class="hljs-string">&#x27;./data&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=transform, download=<span class="hljs-literal">True</span>)<br>dataloader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 定义生成器</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Generator</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Generator, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Linear(z_dimension, <span class="hljs-number">256</span>),<br>            nn.LeakyReLU(<span class="hljs-number">0.2</span>),<br>            nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>),<br>            nn.LeakyReLU(<span class="hljs-number">0.2</span>),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">1024</span>),<br>            nn.LeakyReLU(<span class="hljs-number">0.2</span>),<br>            nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">784</span>),<br>            nn.Tanh()<br>        )<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, z</span>):<br>        img = <span class="hljs-variable language_">self</span>.model(z)<br>        img = img.view(img.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>        <span class="hljs-keyword">return</span> img<br><br><span class="hljs-comment"># 定义判别器</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Discriminator</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Discriminator, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">512</span>),<br>            nn.LeakyReLU(<span class="hljs-number">0.2</span>),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>),<br>            nn.LeakyReLU(<span class="hljs-number">0.2</span>),<br>            nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">1</span>),<br>            nn.Sigmoid()<br>        )<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, img</span>):<br>        img_flat = img.view(img.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>        validity = <span class="hljs-variable language_">self</span>.model(img_flat)<br>        <span class="hljs-keyword">return</span> validity<br><br><span class="hljs-comment"># 初始化模型</span><br>generator = Generator().to(device)<br>discriminator = Discriminator().to(device)<br><br><span class="hljs-comment"># 损失函数和优化器</span><br>criterion = nn.BCELoss()<br>optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(<span class="hljs-number">0.5</span>, <span class="hljs-number">0.999</span>))<br>optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(<span class="hljs-number">0.5</span>, <span class="hljs-number">0.999</span>))<br><br><span class="hljs-comment"># 训练循环</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> i, (real_imgs, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>        real_imgs = real_imgs.to(device)<br>        batch_size = real_imgs.size(<span class="hljs-number">0</span>)<br>        <br>        <span class="hljs-comment"># 创建标签</span><br>        real_label = torch.ones(batch_size, <span class="hljs-number">1</span>).to(device)<br>        fake_label = torch.zeros(batch_size, <span class="hljs-number">1</span>).to(device)<br>        <br>        <span class="hljs-comment"># 训练判别器</span><br>        optimizer_D.zero_grad()<br>        <br>        <span class="hljs-comment"># 真实图像的损失</span><br>        real_pred = discriminator(real_imgs)<br>        d_loss_real = criterion(real_pred, real_label)<br>        <br>        <span class="hljs-comment"># 生成假图像</span><br>        z = torch.randn(batch_size, z_dimension).to(device)<br>        fake_imgs = generator(z)<br>        <br>        <span class="hljs-comment"># 假图像的损失</span><br>        fake_pred = discriminator(fake_imgs.detach())<br>        d_loss_fake = criterion(fake_pred, fake_label)<br>        <br>        <span class="hljs-comment"># 总判别器损失</span><br>        d_loss = d_loss_real + d_loss_fake<br>        d_loss.backward()<br>        optimizer_D.step()<br>        <br>        <span class="hljs-comment"># 训练生成器</span><br>        optimizer_G.zero_grad()<br>        <br>        <span class="hljs-comment"># 生成器希望判别器将假图像判为真</span><br>        fake_pred = discriminator(fake_imgs)<br>        g_loss = criterion(fake_pred, real_label)<br>        <br>        g_loss.backward()<br>        optimizer_G.step()<br>        <br>        <span class="hljs-keyword">if</span> (i+<span class="hljs-number">1</span>) % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch [<span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>], Step [<span class="hljs-subst">&#123;i+<span class="hljs-number">1</span>&#125;</span>/<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(dataloader)&#125;</span>], &quot;</span><br>                  <span class="hljs-string">f&quot;D Loss: <span class="hljs-subst">&#123;d_loss.item():<span class="hljs-number">.4</span>f&#125;</span>, G Loss: <span class="hljs-subst">&#123;g_loss.item():<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># 每个epoch保存生成的图像</span><br>    <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>) % <span class="hljs-number">5</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            test_z = torch.randn(<span class="hljs-number">16</span>, z_dimension).to(device)<br>            generated_imgs = generator(test_z)<br>            generated_imgs = generated_imgs.cpu().numpy()<br>            <br>            <span class="hljs-comment"># 显示生成的图像</span><br>            fig, axes = plt.subplots(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>            <span class="hljs-keyword">for</span> i, ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(axes.flat):<br>                ax.imshow(generated_imgs[i, <span class="hljs-number">0</span>, :, :], cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>                ax.axis(<span class="hljs-string">&#x27;off&#x27;</span>)<br>            plt.savefig(<span class="hljs-string">f&quot;gan_epoch_<span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span>.png&quot;</span>)<br>            plt.close()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training finished!&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>生成对抗网络是深度学习领域最令人兴奋的发展之一，它们不仅推动了人工智能的边界，还为艺术创作、内容生成和数据增强等领域带来了革命性的变化。随着研究的不断深入，我们可以期待GAN在未来发挥更大的作用，创造出更加惊人的成果。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>Goodfellow, I., et al. (2014). Generative Adversarial Nets. NIPS.</li><li>Radford, A., et al. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv:1511.06434.</li><li>Karras, T., et al. (2019). A Style-Based Generator Architecture for Generative Adversarial Networks. CVPR.</li><li>Zhu, J., et al. (2017). Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. ICCV.</li></ol>]]></content>
    
    
    <categories>
      
      <category>DeepLearning</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>DeepLearning</tag>
      
      <tag>Image</tag>
      
      <tag>Neural Networks</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Dify搭建智能体：构建自定义AI应用的实践指南</title>
    <link href="/tech-blog/2024/11/15/Agent-%E5%9F%BA%E4%BA%8EDify%E6%90%AD%E5%BB%BA%E6%99%BA%E8%83%BD%E4%BD%93/"/>
    <url>/tech-blog/2024/11/15/Agent-%E5%9F%BA%E4%BA%8EDify%E6%90%AD%E5%BB%BA%E6%99%BA%E8%83%BD%E4%BD%93/</url>
    
    <content type="html"><![CDATA[<h1 id="基于Dify搭建智能体：构建自定义AI应用的实践指南"><a href="#基于Dify搭建智能体：构建自定义AI应用的实践指南" class="headerlink" title="基于Dify搭建智能体：构建自定义AI应用的实践指南"></a>基于Dify搭建智能体：构建自定义AI应用的实践指南</h1><!-- 图片暂未添加，请后续添加Dify平台图片 --><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>随着大语言模型(LLM)技术的迅速发展，构建自定义AI应用变得越来越重要。Dify作为一个开源的LLM应用开发平台，提供了便捷的工具来创建、部署和管理AI应用。本文将分享如何利用Dify平台构建智能体，无需深厚的编程背景即可打造功能强大的AI应用。</p><h2 id="Dify平台简介"><a href="#Dify平台简介" class="headerlink" title="Dify平台简介"></a>Dify平台简介</h2><p>Dify是一个LLM应用开发平台，支持从构思到部署的全流程开发。它的核心特点包括：</p><ul><li>可视化编排流程，降低开发门槛</li><li>多种模型接入能力，支持OpenAI、Claude等主流大语言模型</li><li>内置知识库和数据集管理</li><li>完善的应用监控和分析功能</li></ul><h2 id="搭建智能体的步骤"><a href="#搭建智能体的步骤" class="headerlink" title="搭建智能体的步骤"></a>搭建智能体的步骤</h2><h3 id="1-环境准备"><a href="#1-环境准备" class="headerlink" title="1. 环境准备"></a>1. 环境准备</h3><p>首先需要注册Dify账号，可以选择<a href="https://cloud.dify.ai/">云服务版</a>或<a href="https://github.com/langgenius/dify">自部署开源版</a>。自部署版需要Docker环境支持。</p><h3 id="2-智能体设计"><a href="#2-智能体设计" class="headerlink" title="2. 智能体设计"></a>2. 智能体设计</h3><p>在创建智能体前，需要明确以下几点：</p><ul><li>智能体的目标和功能边界</li><li>所需的知识库范围</li><li>对话流程设计</li><li>输入&#x2F;输出格式规范</li></ul><h3 id="3-实现步骤"><a href="#3-实现步骤" class="headerlink" title="3. 实现步骤"></a>3. 实现步骤</h3><ol><li>创建应用：在Dify控制台选择”创建应用”，根据需求选择对话或文本生成类型</li><li>配置模型：选择适合的LLM模型（如GPT-4、Claude等）</li><li>编排提示词：设计系统提示和对话引导</li><li>构建知识库：上传相关文档，配置检索参数</li><li>测试与优化：在预览环境中测试智能体表现，根据结果调整参数</li></ol><h2 id="应用案例"><a href="#应用案例" class="headerlink" title="应用案例"></a>应用案例</h2><p>以客服智能体为例，我们可以通过Dify快速实现：</p><ol><li>导入产品文档和FAQ到知识库</li><li>设计对话流程，包括问候、问题解答和转人工环节</li><li>配置模型参数，平衡回答质量和响应速度</li><li>接入网站或应用，提供7*24小时服务</li></ol><h2 id="优化技巧"><a href="#优化技巧" class="headerlink" title="优化技巧"></a>优化技巧</h2><ul><li>提示词工程：精心设计的提示词能显著提高智能体效果</li><li>知识库分类：合理组织知识库，提高检索准确性</li><li>上下文窗口调整：根据应用场景设置合适的上下文长度</li></ul><h2 id="Agent框架对比"><a href="#Agent框架对比" class="headerlink" title="Agent框架对比"></a>Agent框架对比</h2><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ul><li><a href="https://docs.dify.ai/">Dify官方文档</a></li><li><a href="https://github.com/langgenius/dify">Dify GitHub仓库</a></li><li><a href="https://www.promptingguide.ai/">提示词工程指南</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
      <category>开发工具</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>LLM</tag>
      
      <tag>Dify</tag>
      
      <tag>智能体</tag>
      
      <tag>实践</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数字化文档：企业转型的核心驱动力</title>
    <link href="/tech-blog/2023/08/20/%E6%95%B0%E5%AD%97%E5%8C%96%E6%96%87%E6%A1%A3/"/>
    <url>/tech-blog/2023/08/20/%E6%95%B0%E5%AD%97%E5%8C%96%E6%96%87%E6%A1%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="数字化文档：企业转型的核心驱动力"><a href="#数字化文档：企业转型的核心驱动力" class="headerlink" title="数字化文档：企业转型的核心驱动力"></a>数字化文档：企业转型的核心驱动力</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>随着数字化浪潮席卷全球，企业正从传统的纸质文档管理向数字化文档体系转变。数字化文档不仅仅是纸质文档的电子版本，更是企业知识资产的重要载体和数字化转型的关键基础设施。本文将探讨数字化文档的概念、价值、实施策略以及未来发展趋势。</p><h2 id="什么是数字化文档"><a href="#什么是数字化文档" class="headerlink" title="什么是数字化文档"></a>什么是数字化文档</h2><p>数字化文档是指以电子形式存储、管理和分享的文档，包括但不限于：</p><ul><li>通过扫描转换的纸质文档</li><li>原生数字创建的文档（Word、Excel、PDF等）</li><li>结构化数据文档（XML、JSON等）</li><li>富媒体文档（包含音频、视频的复合型文档）</li><li>智能文档（具有交互功能和自动化能力的文档）</li></ul><p>相比传统纸质文档，数字化文档具有易于存储、检索、分享、协作和分析等特点，为企业带来极大便利。</p><h2 id="数字化文档的价值"><a href="#数字化文档的价值" class="headerlink" title="数字化文档的价值"></a>数字化文档的价值</h2><h3 id="提升效率与协作"><a href="#提升效率与协作" class="headerlink" title="提升效率与协作"></a>提升效率与协作</h3><ol><li><strong>实时协作</strong>：多人可同时编辑和评论文档，大幅提升团队协作效率</li><li><strong>远程办公支持</strong>：支持随时随地访问和处理文档，促进灵活工作模式</li><li><strong>流程自动化</strong>：文档审批、流转过程可自动化处理，减少人工干预</li></ol><h3 id="知识管理与保存"><a href="#知识管理与保存" class="headerlink" title="知识管理与保存"></a>知识管理与保存</h3><ol><li><strong>知识沉淀</strong>：将企业经验和智慧以数字形式保存下来</li><li><strong>智能检索</strong>：通过全文搜索和语义分析快速找到所需信息</li><li><strong>版本控制</strong>：跟踪文档变更历史，确保信息准确性</li></ol><h3 id="成本节约"><a href="#成本节约" class="headerlink" title="成本节约"></a>成本节约</h3><ol><li><strong>减少纸张使用</strong>：降低打印和存储成本</li><li><strong>空间节约</strong>：无需大量物理存储空间</li><li><strong>人力资源优化</strong>：减少文档管理的人工成本</li></ol><h3 id="安全与合规"><a href="#安全与合规" class="headerlink" title="安全与合规"></a>安全与合规</h3><ol><li><strong>精细权限控制</strong>：基于角色和需求设置访问权限</li><li><strong>审计追踪</strong>：记录所有文档操作，便于合规审计</li><li><strong>灾难恢复</strong>：数据备份和恢复机制，提高业务连续性</li></ol><h2 id="数字化文档体系建设"><a href="#数字化文档体系建设" class="headerlink" title="数字化文档体系建设"></a>数字化文档体系建设</h2><h3 id="策略与规划"><a href="#策略与规划" class="headerlink" title="策略与规划"></a>策略与规划</h3><ol><li><strong>需求分析</strong>：明确组织对文档管理的需求和挑战</li><li><strong>制定标准</strong>：建立文档命名、分类、元数据等标准</li><li><strong>选择合适工具</strong>：根据业务需求选择文档管理系统</li></ol><h3 id="技术实施"><a href="#技术实施" class="headerlink" title="技术实施"></a>技术实施</h3><ol><li><strong>文档管理系统(DMS)</strong>：核心平台，支持文档全生命周期管理</li><li><strong>内容协作平台</strong>：如Microsoft 365、Google Workspace等</li><li><strong>专业领域工具</strong>：如CAD文档、BIM模型等专业文档管理工具</li><li><strong>集成与互操作</strong>：与企业其他系统（如ERP、CRM）的集成</li></ol><h3 id="流程优化"><a href="#流程优化" class="headerlink" title="流程优化"></a>流程优化</h3><ol><li><strong>文档生命周期管理</strong>：从创建、审批到归档的全流程管理</li><li><strong>工作流自动化</strong>：自动化文档流转和审批过程</li><li><strong>业务流程再造</strong>：基于数字化文档重新设计业务流程</li></ol><h3 id="变革管理"><a href="#变革管理" class="headerlink" title="变革管理"></a>变革管理</h3><ol><li><strong>用户培训</strong>：确保员工掌握新系统和流程</li><li><strong>推广与激励</strong>：鼓励员工积极使用数字化文档</li><li><strong>持续改进</strong>：基于反馈不断优化系统和流程</li></ol><h2 id="数字化文档最佳实践"><a href="#数字化文档最佳实践" class="headerlink" title="数字化文档最佳实践"></a>数字化文档最佳实践</h2><h3 id="文档标准化"><a href="#文档标准化" class="headerlink" title="文档标准化"></a>文档标准化</h3><ol><li><strong>模板管理</strong>：建立统一的文档模板库</li><li><strong>元数据规范</strong>：定义文档属性和标签体系</li><li><strong>文档分类</strong>：建立科学的分类层次结构</li></ol><h3 id="智能内容管理"><a href="#智能内容管理" class="headerlink" title="智能内容管理"></a>智能内容管理</h3><ol><li><strong>OCR技术</strong>：将扫描文档转换为可搜索文本</li><li><strong>智能分类</strong>：利用AI自动对文档进行分类</li><li><strong>内容提取</strong>：自动识别并提取文档中的关键信息</li></ol><h3 id="安全与合规-1"><a href="#安全与合规-1" class="headerlink" title="安全与合规"></a>安全与合规</h3><ol><li><strong>数据加密</strong>：保护敏感文档内容</li><li><strong>水印与防泄漏</strong>：防止文档未授权分享</li><li><strong>合规存档</strong>：满足行业法规对文档保存的要求</li></ol><h2 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h2><h3 id="金融行业"><a href="#金融行业" class="headerlink" title="金融行业"></a>金融行业</h3><p>银行通过数字化文档系统实现了贷款申请流程的无纸化，将审批时间从7天缩短至1天，大幅提升了客户满意度和业务效率。</p><h3 id="制造业"><a href="#制造业" class="headerlink" title="制造业"></a>制造业</h3><p>某制造企业将产品设计文档、工艺文档和质量记录整合到统一的数字化平台，实现了从设计到生产的无缝协作，产品研发周期缩短30%。</p><h3 id="医疗行业"><a href="#医疗行业" class="headerlink" title="医疗行业"></a>医疗行业</h3><p>医院通过电子病历系统管理患者档案，医生可即时访问完整的患者病史，提高了诊断准确性和治疗效果，同时减少了医疗差错。</p><h2 id="未来趋势"><a href="#未来趋势" class="headerlink" title="未来趋势"></a>未来趋势</h2><h3 id="AI驱动的智能文档"><a href="#AI驱动的智能文档" class="headerlink" title="AI驱动的智能文档"></a>AI驱动的智能文档</h3><ol><li><strong>自然语言处理</strong>：理解文档内容，提供智能摘要和分析</li><li><strong>智能问答</strong>：从大量文档中自动提取回答用户问题的信息</li><li><strong>自动生成</strong>：基于数据和模板自动生成标准化文档</li></ol><h3 id="无代码文档应用"><a href="#无代码文档应用" class="headerlink" title="无代码文档应用"></a>无代码文档应用</h3><ol><li><strong>可视化构建</strong>：通过拖拽方式创建文档工作流</li><li><strong>智能表单</strong>：自适应的表单设计，提升数据收集效率</li><li><strong>集成能力</strong>：便捷地与各种业务系统集成</li></ol><h3 id="增强现实-AR-与文档"><a href="#增强现实-AR-与文档" class="headerlink" title="增强现实(AR)与文档"></a>增强现实(AR)与文档</h3><ol><li><strong>交互式说明书</strong>：结合AR技术的产品使用指南</li><li><strong>现场维修指导</strong>：通过AR展示设备维修文档</li><li><strong>空间标注</strong>：将文档信息叠加在物理空间</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>数字化文档已经从简单的电子存储演变为企业数字化转型的核心驱动力。通过建立完善的数字化文档体系，企业不仅能提升运营效率，还能促进知识共享、强化数据安全，并为智能化决策奠定基础。未来，随着人工智能、无代码平台和增强现实等技术的融合，数字化文档将为企业创造更多价值，推动组织迈向真正的数字化转型。</p><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><h3 id="公司"><a href="#公司" class="headerlink" title="公司"></a>公司</h3><p><a href="https://www.bright.cn/">bright data</a></p><h3 id="Github-Projects"><a href="#Github-Projects" class="headerlink" title="Github Projects"></a>Github Projects</h3><h3 id="papers"><a href="#papers" class="headerlink" title="papers"></a>papers</h3><ul><li>AIIM (Association for Intelligent Information Management). (2021). State of the Industry – Content Services.</li><li>Gartner. (2022). Magic Quadrant for Content Services Platforms.</li><li>McKinsey &amp; Company. (2020). The Next Normal: Digitizing at Speed and Scale.</li><li>Deloitte. (2021). Digital Transformation: Powering the Great Reset.</li></ul><h3 id="开源文档"><a href="#开源文档" class="headerlink" title="开源文档"></a>开源文档</h3><p><a href="https://github.com/tesseract-ocr/tesseract">Google Tesseract</a><br><a href="https://github.com/PaddlePaddle/PaddleOCR">Baidu PaddleOCR</a><br><a href="https://github.com/Stirling-Tools/Stirling-PDF">Stirling-PDF</a><br><a href="https://github.com/allenai/olmocr">olmocr</a><br><a href="https://github.com/oomol-lab/pdf-craft">PDF-Craft</a><br><a href="https://github.com/infiniflow/ragflow">RAG-Flow</a></p>]]></content>
    
    
    <categories>
      
      <category>数字化</category>
      
      <category>企业管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数字化转型</tag>
      
      <tag>文档管理</tag>
      
      <tag>企业效率</tag>
      
      <tag>知识管理</tag>
      
      <tag>协作工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
